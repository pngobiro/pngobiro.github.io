Processing PDF: /home/ngobiro/projects/pngobiro.github.io/colleges/kmtc/diploma/Diploma in Health Records and Information Technology/Year 1/Monitoring and Evaluation/notes/Monitoring and Evaluation in Public Health_ Comprehensive Study Notes/Monitoring and Evaluation in Public Health_ Comprehensive Study Notes.pdf
Starting PDF processing...
PDF processing started. ID: 2025_11_19_d8a5a88a857679200a38g
Polling PDF status (ID: 2025_11_19_d8a5a88a857679200a38g)...
Attempt 1: Status = split, Progress = 82.1%
Attempt 2: Status = split, Progress = 94.9%
Attempt 3: Status = completed, Progress = 100.0%
PDF processing completed successfully.
Fetching MMD result (ID: 2025_11_19_d8a5a88a857679200a38g)...

--- Converted MMD Start ---
\title{
© studocu
}

\section*{Monitoring and Evaluation Notes}

\section*{Health records information technology (Kenya Medical Training College)}

Scan to open on Studocu

\section*{MONITORING AND EVALUATION}

By the end of this module, the learner should;
1. Develop an understanding of the project management
2. Establish an understanding of M\&E concepts, methods and processes
3. Establish an understanding of qualitative and quantitative data in monitoring and evaluation
4. Formulate, design, monitoring and evaluation systems
5. Understand financial accounting and management in monitoring and evaluation
6. Develop and implement Performance Monitoring and Evaluation plans
7. Develop skills in report writing and presentation.

\section*{Background Information}

Monitoring and evaluation (M\&E) has gained increasing significance in the health sector during the last decade, partly due to increasing public demand for measurement and accountability in the use of health sector resources. The Constitution of Kenya 2010 and attendant legislation have raised the public's expectations about the ability of national and county governments to put in place measures that increase transparency, accountability and public participation in the implementation of health programs. Both the Kenya Health Policy 2014-2030 and the Kenya Health Sector Strategic Plan 20142018 have integrated the requirements of a robust M\&E system to ensure systematic tracking of investments and progress while promoting a culture of evidence- based planning and decision making. Kenya's Ministry of Health has set out to strengthen M\&E systems within the health sector through a wide range of capacity development initiatives. The Health Sector M\&E Framework 2014-2018 enables all actors to work within convergent efforts to achieve the targets set within the Kenya Health Sector Strategic Plan 2014-2018. Despite these positive milestones, several studies and assessments have documented capacity gaps in implementing a fully functional M\&E system in the health sector.
Increased monitoring and evaluation (M\&E) capacity is vital to ensure that health resources are used effectively. Indeed, functioning M\&E systems will play a major role in evidence-based programming and attracting future resources. In response to rapidly growing demand, the Department of Health Records and Information at Kenya Medical Training College has established a higher diploma program to provide M\&E training and technical support services. This program offers an M\&E course designed to equip Health Records and Information officers and overall health practitioners with the concepts and practical skills required to design and manage effective M\&E systems in all health programs.

The course is intended for Higher diploma students in public health whose future employment may take them into the agencies and government organizations with responsibilities for M\&E planning and implementation, project management, research
work, information management and line management. Such agencies and organizations include:
- Consulting firms specializing in project management, monitoring and evaluation
- National government, provincial and municipal health, epidemiology and HIV/AIDS departments
- International and national NGOs
- Multilateral and bilateral international development assistance agencies

\section*{TOPIC 1: GENERAL M\&E CONCEPTS}

\subsection*{1.1 PROGRAM M\&E RATIONALE}

Public health practitioners (PHP) need knowledge and advanced skills in order to provide various levels of accountability for their activities or policies to a variety of constituencies. It is becoming increasingly important for countries to be able to report accurate, timely and comparable data to national authorities and donors in order to secure continued funding for expanding health programmes and, most importantly, to utilize this information locally to strengthen evolving programmes. This course aims to ensure that PHPs are able to measure, report, and use good quality health and health-related information in a manner that meets both donor and country needs. It is particularly important for national programme implementers and managers to have access to the quality information they need to make adjustments and programmatic and technical decisions (Data for Decision Making).
As such, a common, comprehensive and coherent M\&E system has several advantages: It contributes to more efficient use of data and resources by ensuring, for example, that indicators and sampling methodologies are comparable over time and by reducing duplication of effort. Where resources are scarce, this is an important asset. Data generated by a comprehensive M\&E system ought to serve the needs of many constituents, including programme or project managers, researchers and donors, eliminating the need for each to repeat baseline surveys or evaluation studies when they might easily use existing data.
From the point of view of the national programme, a coherent M\&E system helps ensure that donor-funded M\&E efforts best contribute to national needs. These needs go beyond disease-focused M\&E, rather than simply serving the reporting needs of specific international donors or organizations.
Agreement among the major donor, technical and implementing agencies on the basic core M\&E framework will reduce the burden of requests for data from different agencies. Shared planning, execution, analysis or dissemination of data collection can reduce overlap in programming and increase cooperation between different groups, many of whom may work more efficiently together than in isolation..

\subsection*{1.2 CONCEPTUAL APPROACH AND FRAMEWORK FOR M\&E}

Several considerations underlie the decision-making process about health program monitoring and evaluation. The selection of an appropriate M\&E concept for example an AIDS prevention program is crucial because it determines the guiding philosophy behind the actual monitoring and evaluation process. A number of theorists and M\&E practitioners have proposed various conceptual approaches to M\&E. These approaches
differ in their conception as to what M\&E is, what the relationship with the primary client and other stakeholders should be, who should be making the relevant value judgments regarding the program, and the criteria for judging the evaluation process itself. The conceptual approach debate was, and is for the most part, a debate about the best ways to measure and interpret change. It has highlighted a series of methodological dimensions among which there are variations in emphasis. These dimensions focus attention on some of the options available for making decisions about methods. Today, for example, there is consensus that both quantitative and qualitative data are valued and recognized as legitimate for program M\&E. In fact, these methods are by no means incompatible and should be used in combination. Deciding what and how much data to gather in M\&E involves difficult methodological decisions and trade-offs between the quality and utility of information.
M\&E approach that uses multiple data collection methods, both quantitative and qualitative, is more likely to address diverse evaluation needs than is a more limited approach. At the same time, research priorities must be sensitive to competing needs for resources in an environment in which for example HIV/AIDS epidemic is growing rapidly and evaluation is sometimes considered a luxury. It is a major task of the evaluator to match research methods to the reality of particular M\&E questions and to the available resources. There is also a need for evaluation researchers to play an active role, not merely a consultative one, in making design decisions for program assessments. Although a program M\&E should be a neutral scientific observer, he or she can also mediate between different stakeholder groups, can enable others through a participatory evaluation approach, and can advocate for the dissemination of evaluation results within the larger arena of decision making. Planning M\&E and data collection activities in a participatory fashion is essential for achieving the delicate balance between practical needs and methodological desirability.
Key stakeholders should be included in the planning process and every effort should be made for effective use of limited resources. Ensuring active support and participation of key stakeholders who have an interest in the results obtained by various data collection systems is particularly important for programs funded by external donors that use host country institutions for data collection activities. Data produced by these efforts will have a better chance to be timely and of acceptable quality. Whenever possible, participants, including implementing institutions, host-country collaborators and local representatives of donor agencies, should attempt to reach consensus regarding data needs.

\subsection*{1.3 MONITORING AND EVALUATION}

The term monitoring and evaluation (M\&E) quite broadly includes any effort to increase human effectiveness through systematic data-based inquiry. When one examines and judges accomplishments and effectiveness, one is engaged in monitoring and evaluation. When this examination of effectiveness is conducted systematically and empirically through careful data collection and thoughtful analysis, one is engaged in evaluation research. Thus evaluation is applied research or a type of "action science." This distinguishes evaluation research from basic academic research....The purpose of applied research and evaluation is to inform action, enhance decision-making, and apply knowledge to solve human and societal problems. Applied evaluative research is judged by its usefulness in making human actions and interventions more effective and by its practical utility to decision makers, policymakers and others who have a stake in efforts to improve the world." Such an evaluation approach is utilization focused. This approach
emphasizes the interests of key stakeholders and primary users of the information at all levels, for example the donor, the host country, and the implementing agencies. It applies socio-epidemiological research to identify ways to improve the design and implementation of prevention and care programs. This topic therefore describes several considerations that are fundamental to planning an evaluation effort. It then presents a comprehensive framework for country programs by explaining the major types of evaluation and discussing several important issues related to planning evaluation programs and improving their ability to measure program effects.

\subsection*{1.3.1 Types of a programme or project M\&E are:}
1) Formative assessment and research. Through formative assessment, the program: Identify gaps, define realistic goals/objectives for interventions, and identifies intervention needs and strategies;
2) Monitoring: As a routine process of data collection and measurement of progress toward objects. Three main domains of information required in a monitoring system are inputs, processes, and outputs;
3) Evaluation: Assessing outcomes and impacts and are connected to process monitoring and
4) Cost-effectiveness analysis including sustainability issues.

\subsection*{1.3.2 A comprehensive M\&E framework for country programs}

Prevention and care programs need to be monitored and evaluated at different phases of the program cycle. All stages of evaluation have to be considered together to provide an overall picture of the program because no single data collection approach can supply all the information necessary to improve program performance or affect policy change. Multiple complementary evaluation approaches and multiple methodologies (qualitative and quantitative) have to be applied to address different evaluation needs..

\section*{- Formative Evaluation}

Formative evaluation should be conducted during the planning (or replanning) stage of prevention and care program to identify and resolve intervention and evaluation issues before the program is widely implemented. This is the time when flexibility is greatest and program sponsors are freer to make decisions about how to proceed. Formative evaluation explores the need for interventions, provides the information necessary to define realistic goals and objectives for the program interventions, and helps program planners make tentative decisions about effective, feasible intervention strategies and how to carry them out. Formative evaluation can also be used as an exploratory tool as the project is being carried out to provide feedback to project managers to help them adjust program objectives to changing situations.
Formative evaluation research can identify unacceptable or ineffective intervention approaches, designs, and concepts.. In many cases, interventions have been based on ideas developed outside of the context of the lives of the people to whom the interventions have been delivered. The literature on behavioral change interventions is full of examples of ideas that made perfect sense in the abstract but failed completely in the "real world," mainly because the ideas were unacceptable to the target audience or were not stated in ways that were relevant to the lives of those people. A fuller understanding of the issues might well have led planners to redesign the intervention to make it more appealing to the selected audience. Fortunately, this situation is changing
because formative evaluation is now being applied more frequently in designing prevention programs. Formative evaluations use a mix of research methods that can rapidly provide relevant information to program designers. These methods include:
- Reviews of existing information;
- Focus group discussions;
- Individual in-depth interviews;
- Participant observations; and
- Short quantitative surveys with structured
- Questionnaires

The most frequently cited methodological criticism of formative evaluation is its lack of external validity or generalizability. Because the results of the evaluation derive from small-scale rapid assessment procedures and/or pilot studies, one cannot generalize from them to a larger population. Despite this limitation, formative evaluation research can usually identify unacceptable or ineffective intervention approaches, designs, and concepts. However, even with adequate formative evaluation at the program planning stage, there is no guarantee that a prevention program will be effective when finally implemented; it may not be implemented adequately enough to be effective.

\section*{- Process Evaluation}

Once activities are underway, there is a need to examine whether they are being carried out correctly, on time, and within budget. Process evaluation addresses such basic questions as, "To what extent are planned intervention activities actually realized?" and "What services are provided, to whom, when, how often, for how long, and in what context?" Both input (the basic resources required in terms of manpower, money, material, and time) and output (the immediate service improvement expressed as distributed commodities, trained staff, and service units delivered) are key elements of process evaluation. These questions are often answered in quantitative terms.

Qualitative evidence of how and why a prevention program works or fails to work is equally important in answering process evaluation questions. Process evaluation requires getting close to data, becoming intimately acquainted with the details of the program, and observing not only anticipated effects but also unanticipated consequences. An understanding of the processes through which intervention activities achieve effects can help to explain the outcome of the intervention. Process evaluation, however, does not demonstrate whether interventions are effective. Process evaluation can also play an important role in improving or modifying interventions by providing the information necessary to adjust delivery strategies or program objectives in a changing epidemic. Process-oriented evaluation is carried out throughout the course of the program implementation and should use different methodological approaches to assess service delivery, ranging from reviews of service records and regular reporting systems, key informant interviews, exit interviews of service users, direct observations by 'mystery clients' (for example, in sexually transmitted infection [STI] and voluntary counseling and testing [VCT] services) to quantitative population-based surveys to assess program coverage and barriers to service use. Different qualitative and quantitative study designs that are complementary to one another provide together the most comprehensive information.

\section*{- Effectiveness (Efficiency and Accountability) Evaluation: Assessing Outcome and Impact}

Evaluating the effectiveness of a disease prevention programs will almost always require quantitative measurements. These measurements will assess the extent to which the objectives of the program were achieved. Effectiveness evaluation is used to answer the questions, "What outcomes were observed?," "What do the outcomes mean?," and "Does the program make a difference?" Taking into account the various implementation stages of HIV/AIDS prevention programs and the fact that, over time, new age cohorts become sexually active, it is advisable to stratify effectiveness evaluation by short-term and intermediate program effects (program outcome) and long-term program effects (program impact).

Table below provides examples of program outcome and impact measures for these different stages.

\begin{tabular}{|l|l|}
\hline Program Outcome (short-term and intermediate effects) & Program Impact (long-term effects) \\
\hline Changes in HIV/AIDS-related attitudes & Sustained changes in HIV/STI-related risk behaviors \\
\hline HIV/STI-related risk behaviors & Trends in HIV/AIDS rates \\
\hline Trends in STI rates (e.g., gonorrhea & AIDS-related mortality rates \\
\hline Increase in social support/community response & Reduced individual and societal vulnerability to HIV/AIDS \\
\hline & Sustained changes in societal norms \\
\hline
\end{tabular}

Changes in HIV/AIDS-related attitudes, the reduction of risk behaviors and adoption of protective behaviors, and changes in STI rates are considered to be the most appropriate short-term or intermediate (also called proximate) outcome measures for interventions designed to reduce sexual transmission of HIV. Long-term effects include impact on HIV/AIDS trends, sustainability issues, and improved societal response. Outcome and impact evaluation is intimately connected with process evaluation. Process information can help the evaluator to understand how and why interventions have achieved their effects and, perhaps, what is actually making the difference. Examining outcome/impact indicators without assessing the process of program implementation could lead to erroneous conclusions regarding the effectiveness of the intervention. Program goals and objectives have to be carefully defined to allow the selection of appropriate outcome and impact measures to assess the effectiveness of an AIDS prevention program. Effectiveness evaluation is generally based on indicators that provide quantitative value from which the outcome and impact of interventions can be measured. Because multiple interventions working synergistically together are most effective in producing behavior change, surveys should not be typically designed to capture the effects of one single intervention. Rather, they should be designed to measure behavioral trends in population groups who are exposed to combined interventions. The evaluation of one intervention is usually conducted through rigorous and expensive controlled trials.
- Cost-effectiveness Analysis

Evaluating the effectiveness of prevention programs will almost always require quantitative measurements. These measurements will assess the extent to which the objectives of the program were achieved. Effectiveness evaluation is used to answer the questions, "What outcomes were observed?," "What do the outcomes mean?," and "Does the program make a difference?" For example taking into account the various implementation stages of HIV/AIDS prevention programs and the fact that, over time, new age cohorts become sexually active, it is advisable to stratify effectiveness
evaluation by short-term and intermediate program effects (program outcome) and long-term program effects (program impact).

Outcome and impact evaluation is intimately connected with process evaluation. Process information can help the evaluator to understand how and why interventions have achieved their effects and, perhaps, what is actually making the difference. Examining outcome/impact indicators without assessing the process of program implementation could lead to erroneous conclusions regarding the effectiveness of the intervention.

Program goals and objectives have to be carefully defined to allow the selection of appropriate outcome and impact measures to assess the effectiveness of an AIDS prevention program. Effectiveness evaluation is generally based on indicators that provide quantitative value from which the outcome and impact of interventions can be measured. Because multiple interventions working synergistically together are most effective in producing behavior change, surveys should not be typically designed to capture the effects of one single intervention .Rather, they should be designed to measure behavioral trends in population groups who are exposed to combined interventions. The evaluation of one intervention is usually conducted through rigorous and expensive controlled trials.
Cost-effectiveness analysis also measures program effectiveness, but expands the analysis by adding a measure of program cost per unit of effect. By comparing the costs and consequences of various interventions, cost analyses and cost effectiveness estimates can assist in priority setting, resource allocation decisions, and program design. The attribution dilemma: are observed changes a result of prevention interventions?
Program evaluation is intrinsically complex, however, due to the temporal evolution of epidemics and our poor understanding of how different behaviors and epidemiologic factors influence epidemic patterns as they move from an epidemic phase to an endemic state. Several factors unrelated to intervention effects can contribute to the observed stabilization or decreases in the prevalence or incidence of diseases in a given setting. They include:
- Mortality, especially in mature epidemics;
- Saturation effects in populations at high risk;
- Behavioral change in response to the experience of disease among friends and relatives;
- Differential migration patterns related to the epidemic; and
- Sampling bias and/or errors in data collection and analysis.

Determining whether observed changes in disease incidence and prevalence are a reflection of the natural history of the epidemic or due to intervention effects is a critical evaluation issue. This is particularly true when evaluating behavior changes in the face of growing numbers of people with AIDS-related illnesses because there is evidence that secular trends toward risk reduction will occur. For example, having a friend or relative with HIV/AIDS may influence adolescents to delay the onset of sexual relations or motivate those with non-regular sex partners to use condoms. Their different perspectives on this issue also reflect fundamental differences regarding the criteria for judging the process of program evaluation itself. From a public health perspective, it may not matter whether the observed changes are due to a particular intervention. From the costeffectiveness or policy perspective, however, it is important to determine what caused the
observed changes in sexual behavior. If the changes would have occurred without a particular intervention that was designed to contribute to the observed changes, the costs of the intervention could be considered as resources better spent on something more useful. Prevention programs are under growing pressure to estimate which approaches work best for specific target populations in different epidemiologic settings with a given level of inputs in order to allocate resources in a cost-effective manner.
Effectiveness evaluation, therefore, is critical because it can answer a basic question, "Does the program make a difference?" A vexing task of assessing program effectiveness is to disentangle the attributable affects of a prevention program from the gross outcome and impact observed. Such estimates can be made with varying degrees of plausibility, but not with certainty. A general principle applies here: The more rigorous the research design, the more convincing the resulting estimate. A hierarchy of evidence based on the study design can be established that reflects the degree of certainty in concluding that a given proportion of the observed changes in behavior is attributable to the intervention program and is not the result of other factors.
The interpretation of program evaluation data should always be approached with caution. In most situations, the program and evaluation process as a whole is not a rigorously controlled experimental trial. The ability of an evaluation to precisely determine the true extent of a program's effectiveness is often limited by time, resources, and the lack of a rigorous design. Many factors can confuse or confound the results measured, and biases can be introduced by a range of factors inherent to the problem of HIV/AIDS, the available measurement options, and those conducting the evaluation. One of the most difficult questions to answer in any evaluation is that of attributing any measured effect to the program being evaluated. Defining the web of interacting and overlapping influences is extremely difficult, and is one of the reasons why so many programs have difficulty attributing results to their actions. At some point, we need to stop worrying about attribution in such settings and focus on monitoring the changes as they occur.

\subsection*{1.3.3 What is the difference between national and sub-national M\&E?}

In view of scarce M\&E resources at sub-national level, emphasis is placed on monitoring programme inputs and outputs and assessing whether or not implementation progresses according to a sub-national plan. A small facility assessment as part of a routine supervision could serve to provide information on the quality of care or the availability and utilization of services. At all levels, both monitoring and evaluation are needed. Subnational data is extremely relevant for national level M\&E provided that national guidelines are followed to make aggregation possible. Information gathered from the subnational level is helpful in guiding policy discussions and in validating results at higher levels. In some cases, data from the sub-national provides a better indication of trends. For example, if a country has actual data on condom distribution by district (or equivalent) instead of one national overall figure, monitoring of trends in condom use may become more meaningful and more accurate.

\subsection*{1.3.4 What is the difference between programme and project M\&E?}

Programme refers to an overarching national or sub-national response to the disease. Within a national programme, there are typically a number of different areas of programming. For example, the HIV/AIDS programme has a number of "subprogrammes or projects" such as blood safety, STI control, or HIV prevention for young people.

Project refers to a mix of interventions with activities supported by resources that aim at a specific population defined geographically or otherwise. It should be noted that projects and programmes can also be defined by timeframes - projects are usually short term where as programmes are usually longer term in scope. In view of its wider scope (thematic, geographic, target population), programme monitoring tends to be more complex than project monitoring and requires strong coordination among all implementing agencies.
Programme evaluation is even more difficult, especially for certain types of evaluations (outcome and impact evaluations). For such evaluations to be conducted, the design of the programme/project must include its own baseline and follow-up assessments measuring not only specific outcomes but also the level of exposure to the programme/project and its activities. (See question 4 for more details on evaluations)

\subsection*{1.3.5 Monitoring (M)}

Monitoring is the routine tracking of the key elements of programme/project performance, usually inputs and outputs, through record-keeping, regular reporting and surveillance systems as well as health facility observation and client surveys. Monitoring helps programme or project managers determine which areas require greater effort and identify areas that might contribute to an improved response. In a well-designed monitoring and evaluation system, monitoring contributes greatly towards evaluation. Indicators selected for monitoring will be different depending on the reporting level within the health system. It is very important to select a limited number of indicators that will actually be used by programme implementers and managers. There is a tendency to collect information on many indicators and report this information to levels where it will not and cannot be used for effective decision-making.

\subsection*{1.3.6 Evaluation (E)}

In contrast to monitoring, evaluation is the episodic assessment of the change in targeted results that can be attributed to the programme or project/project intervention. In other words, evaluation attempts to link a particular output or outcome directly to an intervention after a period of time has passed. Evaluation helps programme or project managers determine the value or worth of a specific programme or project. Costeffectiveness and cost-benefit evaluations are useful in determining the added value of a particular programme or project. Evaluation aims at assessing outcome and impact of a program.

The timing for a specific type of evaluation depends on the implementation status of a programme or project.

\section*{There are four types of programme or project evaluations:}
- Formative evaluation
- Process evaluation
- Outcome evaluation
- Impact evaluation

Formative evaluation is conducted in the design phase of prevention and care programme to identify and resolve intervention and evaluation issues before the
programme is widely implemented. Formative evaluation identifies transmission dynamics assists in identifying effective interventions and helps define realistic goals.

Process evaluation involves the assessment of the programme or project's content, scope or coverage together with the quality of implementation. If the process evaluation finds that the programme/project has not been implemented, or is not reaching its intended audience, it is not worth conducting an outcome evaluation. However, if process evaluation shows progress in implementing the programme/project as planned, then it is worth carrying out such an evaluation.

Outcome evaluation is designed specifically with the intention of being able to attribute the changes to the intervention itself. At the very least, the evaluation design has to be able to plausibly link observed outcomes to a well-defined programme or project, and to demonstrate that changes are not the result of non-programme/project factors.

Impact evaluation: If the evaluation shows a change in outcomes, then it is time for impact evaluation. True impact evaluation, able to attribute long-term changes to a specific programme or project, is very rare and quite costly. Rather, monitoring impact indicators taken in conjunction with process and outcome evaluations are considered to be sufficient to indicate the overall impact.

As seen above, the objectives and the methodology used in monitoring and evaluation are different. In general, evaluations are more difficult in view of the methodological rigor needed; without such rigor, wrong conclusions on Monitoring and Evaluation toolkit the value of a programme or project can be drawn. They are also more costly, especially outcome and impact evaluations which require population-based surveys.

\subsection*{1.3.7 M\&E Indicators}

Indicators are markers designed to monitor and evaluate program performance/ effectiveness or goals/objectives. As illustrated below, types are input, output, outcome and impact indicators based on life cycle of a program. Ideal indicator should have the following seven characteristics:

✓ Valid-They should measure the condition or event they are intended to measure.
✓ Reliable-They should produce the same results when used more than once to measure the same condition or event.
✓ Specific-They should measure only the condition or event they are intended to measure.
✓ Sensitive-They should reflect changes in the state of the condition or event under observation.
✓ Operational-It should be possible to measure or quantify them with developed and tested definitions and reference standards.
✓ Affordable-The costs of measuring the indicators also should be reasonable.
✓ Feasible-It should be possible to carry out the proposed data collection.
Validity is inherent in the actual content of the indicator and also depends on its potential for being measured. Reliability is inherent in the methodology used to measure the indicator and in the person using the methodology. Interpreting outcome indicators for behavioral interventions that promote safer behaviours is further complicated by the fact
that risk behaviors are measured in relative terms. For example, percentage figures of condom use measure the proportion of sexual exposures that are considered to be safe. These may or may not reflect the absolute number of sex acts that place individuals at risk for exposure to sexual transmission. Ten percent condom use in 10 HIV-associated sexual episodes is still "safer" than 75 percent condom use in 100 HIV-associated sex episodes ( 9 versus 25 unprotected HIV-associated sex acts, respectively). Therefore, in this example it also would be important to determine the frequency of condom use in absolute terms in a given risk situation. Behavioral surveys have begun to address this dilemma by collecting additional data on "always or consistent" condom use in the context of sexual episodes with non-regular partners.
Indicators are used at different levels to measure what goes into a programme or project and what comes out of it. Over the past few years, one largely agreed upon framework has emerged, the input-process-output-outcome-impact framework illustrated below.
$$\text { Input ⟶ process } \text { ⟶ } \text { output } \text { ⟶ } \text { outcome } \text { ⟶ } \text { impact }$$

For a programme or project to achieve its goals, inputs such as money and staff time must result in outputs such as stocks and delivery systems for drugs and other essential commodities, new or improved services, trained staff, information materials, etc. These outputs are often the result of specific processes, such as training sessions for staff, that should be included as key activities aimed at achieving the outputs. If these outputs are well designed and reach the populations for which they were intended, the programme or project is likely to have positive short-term effects or outcomes, for example increased condom use with casual partners, increased use of insecticide-treated nets (ITNs), adherence to TB drugs, or later age at first sex among young people. These positive short-term outcomes should lead to changes in the longer term impact of programmes, measured in fewer new cases of HIV, TB, or malaria. In the case of HIV, a desired impact among those infected includes quality of life and life expectancy.

Measuring impact requires extensive investment in evaluation, and it is often difficult to ascertain the extent to which individual programmes, or individual programme components, contribute to overall reduction in cases and increased survival. In order to establish a cause-effect relationship for a given intervention, studies with experimental or quasi experimental designs may be necessary to demonstrate the impact. Outputs or outcome indicators however, can also be used to a certain degree to identify such relationships and can give a general indication of programmes progress according to agreed upon goals and targets.

✓ Generic input indicator: Existence of national policies, guidelines, or strategies. This is a "yes" / "no" question. Reporting of overall budget allocation is included as an input.
✓ Generic process indicator: Number of persons trained, number of drugs shipped/ordered, etc. In the case of HIV/AIDS, for example, these include prevention, treatment, care and

\section*{Examples of cross cutting indicators for HIV/AIDS and TB Programmes}

\section*{Health systems strengthening}
- Number of project staff trained
- \% of project budget spent on health infrastructure
-\% of project beneficiaries (patients) who are accurately referred
Coordination and partnership development
- Number of networks/partnerships involved in project

Monitoring and evaluation
- Number of project service deliverers trained in M\&E
- \% of overall project budget spent on M\&E

Procurement and supply management capacity building
- Number of project services deliverers trained in procurement and supply management
- \% of project service delivery points with sufficient drug supplies
- Unit cost(s) of project drug(s) and commodities

\section*{- Standard Indicators}

\section*{Why do we need standard indicators?}

The use of standard indicators provides the National Programme with valuable measures of the same indicator in different populations, permitting triangulation of findings and allowing regional or local inconsistencies and differences to be noted and addressed. This helps to direct resources to regions or sub-populations with greater needs and to identify areas for intensification or reduction of effort at the national level, ultimately improving the overall effectiveness of the national response. The use of standard indicators also ensures comparability of information across countries and over time.
In designing their own evaluation activities, projects should also bear in mind the national standard for indicators in that field. Projects may have their own information needs that conform to a rigorous evaluation design. However, whenever possible they should choose indicators with standard references, e.g. reference periods, numerators, denominators collected consistently over various time periods that would allow the data they collect to be fed easily into the national M\&E system, and compared over time.

\section*{- Choices of indicators}

One of the critical steps in designing and carrying out M\&E of any program for that matter is selecting appropriate indicators. This can be a fairly straightforward process if the objectives of the program have been clearly stated and presented in terms that define quantity, quality, and time frame of a particular aspect of the program. Even with welldefined objectives, however, the choice of indicators for the evaluation of many programs requires careful thought and consideration of both theoretical and practical elements. The following questions can be helpful in selecting indicators:
- Is the focus of the objective a parameter that can be measured accurately and reliably?
- Are there alternative measures that need to be considered?
- What resources (human and financial) does the indicator require?
- Are there areas for congruency, either in the content of the indicator or the means of gathering the data?
- Are there any additional measures that would help in interpreting the results of the primary objective?

Selecting indicators and setting targets is usually done during the process of program planning and replanning, preferably in a participatory way with the implementing agency and key stakeholders. Setting targets and benchmarks should also include information from similar types of interventions, so that the targets set are realistic from the perspective of the target population, resource allocation, and intervention type. While the level of attainment to be measured by the indicator is not actually part of the indicator itself, it is a critical factor. The magnitude of the level to be measured affects the size of the sample of the population needed to estimate that level accurately. It may also help evaluators select additional or supplemental indicators that might assist in later interpretations of the results.
The choice of indicators should be therefore driven by the goals of the national programme or project. There is no point in collecting data on areas that are not relevant to the local context, bearing in mind that it costs time and money to collect and analyze data for each indicator.

\section*{- Guiding principles in choosing a set of indicator}

The following guiding principles help in choosing the most appropriate set of indicators and associated data collection instruments:
1. Use a conceptual framework for M\&E for proper interpretation of the results (see above for suggested framework);
2. Ensure that the indicators are linked to the programme or project goals and are able to measure change;
3. Ensure that standard indicators are used to the extent possible for comparability over time and between countries or population groups;
4. Consider the cost and feasibility of data collection and analysis; and
5. For HIV/AIDS, take into account the stage of the epidemic
6. Keep the number of indicators to the minimum needed, with specific reference to the level of the system that require and will use which indicators to make programming and management decisions. Additional indicators can always be identified later.

The Table below lists possible indicators related to different levels of program evaluation. The advantage of relating indicators to specific evaluation levels is that it also helps to identify opportunities for triangulating data.

\begin{tabular}{|l|l|}
\hline \multicolumn{2}{|l|}{Examples of indicators by level of program evaluation} \\
\hline Levels of Evaluation & Indicator \\
\hline 1. Process Evaluation & \\
\hline INPUTS & \begin{tabular}{l}
- Resources allocated (e.g., percent of national budget) \\
- Condom availability at central level (Prevention Indicator
\end{tabular} \\
\hline OUTPUTS & \begin{tabular}{l}
- Knowledge of HIV transmission (PI 1) \\
- Condom availability in periphery (PI 3) \\
- Proportion of those 12-17 years of age receiving AIDS/sexual health education \\
- Percentage of services with improved quality, e.g., STI case management (PI 6 and \\
PI 7), voluntary counseling and testing (VCT), care for people living with HIV/AIDS \\
- Percentage of blood transfusion facilities with uninterrupted supply of appropriate \\
HIV screening tests
\end{tabular} \\
\hline 2. Effectiveness Evaluation & \\
\hline OUTCOMES & \begin{tabular}{l}
- Condom use during last act with non-regular partner \\
- Prevalence of urethritis among men aged 15-49 in last year \\
- Prevalence of positive syphilis (RPR/VDRL) serology among antenatal women aged 15-24
\end{tabular} \\
\hline IMPACT (HIV incidence/prevalence/mitigation) & \begin{tabular}{l}
- HIV prevalence among women less than 25 years of age in antenatal clinics (to approximate incidence) \\
- HIV prevalence among high risk groups, e.g., STI patients/sex workers/injecting \\
drug users \\
- HIV prevalence among adult men aged 15-49 \\
- HIV-associated mortality rates among adults aged 15-59 \\
- Number of communities with increased coping capacity
\end{tabular} \\
\hline
\end{tabular}

\section*{RATIONALE, NUMERATOR AND DENOMINATOR OF COVERAGE INDICATORs}

\section*{✓ Definition}

Indicators are markers designed to monitor and evaluate program performance/ effectiveness or goals/objectives. Indicator types are: input, output, outcome and impact based on life cycle of a program.

\section*{Seven characteristics of an ideal indicator are:}
1. Directly related to program objectives,
2. Operational (measurable),
3. Reliable
4. Valid
5. Specific,
6. Sensitive
7. Affordable and feasible.

\section*{- Significance of standard indicators}

The use of standard indicators provides the National Programme with valuable measures of the same indicator in different populations,

✓ permitting triangulation of findings and
✓ allowing regional or local inconsistencies and differences to be noted and addressed.
✓ ensures comparability of information across countries and over time.
This helps to direct resources to regions or sub-populations with greater needs and to identify areas for intensification or reduction of effort at the national level, ultimately improving the overall effectiveness of the national response. In designing their own evaluation activities, projects should therefore bear in mind the national standard for indicators in that field. Projects may have their own information needs that conform to a rigorous evaluation design. However, whenever possible they should choose indicators with standard references, e.g. reference periods, numerators, denominators collected consistently over various time periods that would allow the data they collect to be fed easily into the national M\&E system, and compared over time.

\section*{- Coverage indicators}

\section*{Example 1: Percentage of sex workers reached with HIV prevention programs (HIV-C-P2) for HIV prevention among sex workers.}

\section*{Rationale}

It measures progress in implementing basic elements of HIV prevention programmes for sex workers. Sex workers are often difficult to reach with HIV prevention programmes. However, in order to prevent the spread of HIV and AIDS among sex workers as well as into the general population, it is important that they access these services. Note: Countries with generalized epidemics may also have a concentrated sub-epidemic among one or more most-at-risk populations. If so, they should calculate and report this indicator for those populations.

\section*{Numerator}

Number sex workers who replied "yes" to both questions:
1. Do you know where you can go if you wish to receive an HIV test?
2. In the last twelve months, have you been given condoms?

\section*{Denominator}

Total number of sex workers surveyed

\section*{Measurement}

Behavioural surveillance or other special surveys. Respondents are asked the following questions:
1. Do you know where you can go if you wish to receive an HIV test?
2. In the last twelve months, have you been given condoms? (e.g. through an outreach service, drop-in centre or sexual health clinic)

Scores for each of the individual questions-based on the same denominator-are required in addition to the score for the composite indicator. Whenever possible, data for sex workers should be collected through civil society organizations that have worked closely with this population in the field. Access to survey respondents as well as the data collected from them must remain confidential. This indicator only covers two basic elements of prevention programmes for sex workers. It is recognized that the indicator does not measure the frequency with which members of these populations access services, nor the quality of these services. These limitations suggest that the indicator may overestimate the coverage of HIV prevention services or sex workers. While continued monitoring of this indicator is recommended in order to determine trends in coverage of minimum services, additional measures are required in order to accurately determine whether adequate HIV prevention services are being provided for these populations.

\subsection*{1.3.8 Goals and objectives of Health programs' M\&E system}

The goals and objectives of HIV/TB program are the core of any M \&E system because: (i) M\&E is a process of data collection and measurement of progress/impact toward program goals/objectives while evaluation is a systematic investigation of a program's effectiveness in line with the objectives (ii) Goals or objectives are therefore an essential component in quantifying the aims of HIV/AIDS \& TB related policies, programs and services through M\&E systems. Five features of smart objectives in a project are: $\mathrm{S}=$ Specificity, $\mathrm{M}=$ Measurability, $\mathrm{A}=$ Attainability, $\mathrm{R}=$ Reliability and $\mathrm{T}=$ Time bound

\subsection*{1.3.9 Data Collection}

\section*{- Data vs Information}

A mixed qualitative and quantitative approach should be applied when collecting data and analyzing information. The mixed methodological approach will contribute to a more substantial understanding of programme progress, ensure triangulation of data sources and reduce biases in the data.
Different methods of collecting data for M\&E: These include quantitative \& qualitative methods / related instruments or tools = questionnaires, observations/checklist, FGDs, etc. All methods require participatory approach.

Planning data collection for selected indicators requires different strategies because:
✓ The cost, difficulty, and capacity required for collecting information increases as indicators shift from input through outputs and from outcomes to impact.
✓ The reverse is true when assessing the impact of program related interventions, which decrease as indicators move from input to impact.
✓ Input and output indicators are often easy and less costly to collect compared to outcome and impact indicators.
✓ Data for input and output indicators are centrally collected from regular health monitoring systems, provided that such systems are functional. While outcome and impact indicator data are collected through more costly and difficult population-based or health facility surveys, requiring some expertise in research methods. Based on
these reasons, planning data collection for selected indicators requires different strategies.

\section*{Optimizing the use of data for any disease programming}

The ultimate goal of data collection is to ensure that data are fed back into the decisionmaking process. In addition, data are powerful tools for advocacy, generating resources, accountability, program design and improvement, and attributing changes to specific interventions. The following steps can help to optimize the use of data:
1) Produce quality data;
2) Identify the different end-users and present and package the data according to their needs, focusing on a minimal number of indicators at each level;
3) Set up mechanism for an efficient data-use system, including feedback through supervision at all levels an assurance that data at given level is relevant and actionable at that level;
4) Ensure ownership throughout the data collection exercise, which means that national and local M\&E capacities must be strengthened to guarantee uniform and quality data within a sustainable framework;
5) Ensure that an M\&E support group with strong presence from the government, donor agencies, NGOs, and academic institutions is established to guide the government throughout the development and implementation of national M\&E strategies. This will improve the credibility of the data generated by the government and
6) Allocate sufficient resources for the development and implementation of a data use-plan

\subsection*{1.4 M\&E SYSTEM \& FRAMEWORK}

\subsection*{1.4.1 M\&E System}

On 25 April 2004, the representatives of major donor organizations and of many developing countries adopted " The three "ones" principles as the overreaching framework to better coordinate the scale-up of National Programmes and related responses to epidemics. The "Three Ones Principles" are:
- One agreed action framework that provides the basis for coordinating the work of all partners;
- One national coordinating authority, with a broad based multi-sector mandate; and
- One agreed upon country-level monitoring and evaluation system.

\section*{Importance of creating, implementing and strengthening a single, unified and coherent M\&E system at the country level cannot be overemphasized.}

A strong unified M\&E system ensures that: 1) relevant, timely and accurate data are made available to programme leaders and managers at each level of the programme and health care system; 2) selected quality data can be reported to national programme leaders; and 3) the national programme is able to meet donor and international reporting requirements under a unified global effort to contain the HIV/AIDS pandemic. As a programme or intervention matures, users may consider evaluating impact, using information that has
been collected through the programme's life and/or by undertaking special evaluation studies.

\begin{tabular}{|l|l|}
\hline \multicolumn{2}{|l|}{Features of a Good M\&E system} \\
\hline M\&E UNIT & \begin{tabular}{l}
- An established M\&E unit within the Ministry of Health with designated technical and data management staff. This unit should, among other things, coordinate M\&E efforts across the three disease areas, irregardless of where individual diseasespecific M\&E is housed. \\
- A budget for M\&E that is between 5 and 10 percent of the combined national budgets from all sources. On average, $7 \%$ should be used as the reference. \\
- A significant national contribution to the national M\&E budget (not total reliance on external funding sources) \\
- A formalized (M\&E) link, particularly with appropriate line ministries, NGOs and donors, and national research institutions aimed at enhancing operations research efforts \\
- A multi-sectoral working group to provide input and achieve consensus on indicator selection and various aspects of M\&E design and implementation \\
- Epidemiological expertise in the M\&E unit or affiliated with the unit \\
- Behavioral/social science expertise in the M\&E unit or affiliated with the unit \\
- Data processing and statistical expertise in the M\&E unit or affiliated with the unit \\
- Data dissemination expertise in the M\&E unit or affiliated with the unit \\
- Expertise in resource tracking, including both financial and commodity resources
\end{tabular} \\
\hline CLEAR GOALS & \begin{tabular}{l}
- Well-defined national programme or project plans with clear goals, targets and operational plans. National M\&E plans should be revised every 3-5 years, and M\&E operational plans yearly. \\
- Regular reviews/evaluations of the progress of the implementation of the national \\
programme or project plans \\
- Guidelines and guidance to districts and regions or provinces for M\&E \\
- Guidelines for linking M\&E to other sectors such as education, labor, and military. \\
- Coordination of national and donor M\&E needs
\end{tabular} \\
\hline INDICATORS & \begin{tabular}{l}
- A set of priority indicators and additional indicators at different levels of M\&E \\
- Indicators that are comparable over time \\
- A number of key indicators that are comparable with other countries
\end{tabular} \\
\hline DATA COLLECTION \& ANALYSIS & \begin{tabular}{l}
- An overall national level data collection and analysis plan, including data quality assurance \\
- A plan to collect data and periodically analyze indicators and associated data sets at \\
different jurisdictional levels of M\&E (including geographical) \\
- Second Generation Surveillance, where behavioral data are linked to HIV/STI surveillance \\
Data
\end{tabular} \\
\hline DATA DISSEMINATION & \begin{tabular}{l}
- An overall national level data dissemination plan \\
- A well-disseminated, informative annual report of the M\&E unit \\
- Annual meetings to disseminate and discuss M\&E and research findings with
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|l|l|}
\hline & \begin{tabular}{l}
policy makers, planners and implementers \\
- A clearinghouse for generation and dissemination of findings \\
- A centralized database or library of all program-related data collection, including ongoing research \\
- Coordination of national and donor M\&E dissemination needs
\end{tabular} \\
\hline
\end{tabular}

\subsection*{1.4.2 M\&E Framework}

\section*{- The 4 components of a comprehensive M\&E framework are:}
1. Formative assessment and research. Through formative assessment, the program: Identify gaps, define realistic goals/objectives for interventions and identifies intervention needs and strategies;
2. Monitoring: As a routine process of data collection and measurement of progress toward objects. Three main domains of information required in a monitoring system are inputs, processes, and outputs;
3. Evaluation: Assessing outcomes and impacts and are connected to process monitoring and
4. Cost-effectiveness analysis including sustainability issues. All these help managers and planners to make decisions about the use of their resources at all levels.

\begin{tabular}{|l|l|l|l|}
\hline \multicolumn{4}{|l|}{The M\&E framework, with example areas, key questions, and indicators} \\
\hline Level & Area & Key Questions & Indicator example \\
\hline INPUT (strategies, policies, guidelines, financing) & \begin{tabular}{l}
- Policy \\
- Disbursement \\
- Infrastructure \\
- Coordination
\end{tabular} & \begin{tabular}{l}
- National strategic plans for each disease and related areas (i.e OVCs), including M\&E and operations research plans exist \\
- Policy and guidelines exist \\
- Coordination is established \\
- Infrastructure and equipment available
\end{tabular} & \begin{tabular}{l}
- Policy and guidelines in place at national level \\
- Distribution node selected \\
- Sentinel site selected \\
- Providers selected \\
- Coordination mechanism in place for technical and operational issues
\end{tabular} \\
\hline \multirow[t]{2}{*}{PROCESS (human resources, training, commodities)} & - Human resources & - Human resources for service delivery and supervision are recruited, adequately motivated, trained and deployed & - Number of people trained according to national standards for an intervention \\
\hline & - Drugs, basic needs, and commodities & \begin{tabular}{l}
- Drugs are consistently available to consumers at the right time and place \\
- Basic needs (food, clothes, etc.) are consistently available to vulnerable populations at the right time and place \\
- Standard treatment guidelines and utilization manuals have
\end{tabular} & \begin{tabular}{l}
$\bullet \%$ of drug distribution nodes reporting on stock status (repletion, shortage, consumption, quality, losses) on a monthly basis \\
- Number of activities organized which address basic needs of vulnerable populations
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|l|l|l|l|}
\hline & & been developed and produced & - Treatment guidelines and utilization manuals developed and available at service sites \\
\hline OUTPUTS (services, numbers reached, coverage) & - Service delivery, technologies & - Intervention is accessible in a large number or majority of districts or other administrative unit & \begin{tabular}{l}
- Number or \% of districts or other administrative unit with at least one drug distribution center \\
- Number or \% of districts or other \\
administrative unit with the required number of providers of the intervention \\
- \% of drug distribution nodes/facilities reporting no drug shortage \\
- \% of selected providers equipped for the intervention (laboratories, nursing, psychosocial support, others)
\end{tabular} \\
\hline & - Knowledge, skills and practice & \begin{tabular}{l}
- Target population knows about the benefi $t$ of the intervention \\
- Target population has improved knowledge and attitudes to diseaeses
\end{tabular} & - Number or \% of districts or other administrative unit with designated sentinel/ provider operating according to guidelines for the intervention \\
\hline \begin{tabular}{l}
OUTCOMES \\
(changed behaviours, coverage)
\end{tabular} & - People on treatment, people benefiting from intervention & \begin{tabular}{l}
- A majority of target population is \\
covered by the intervention
\end{tabular} & - Number or \% of target population covered by intervention \\
\hline & - Changed behaviour & - Increased number or proportion of target population adopting behaviours which reduce their vulnerability to infection, morbidity, and/or mortality & - Number or \% of target population with desired health seeking behaviour (risk reduction, health care seeking) \\
\hline \begin{tabular}{l}
IMPACT \\
(biology and quality of life)
\end{tabular} & - Morbidity, mortality, socio-economic well-being & - Majority of target population is in better health and well-being as a result of the intervention & \begin{tabular}{l}
- Number of target population showing clinical (and measurable) signs of recovery after 6, 12 months \\
- \% of people showing clinical (and measurable)
\end{tabular} \\
\hline
\end{tabular}

\begin{tabular}{|l|l|l|l|}
\hline & & & \begin{tabular}{l} 
signs of recovery after 6, \\
12 months
\end{tabular} \\
\hline
\end{tabular}

\subsection*{1.4.3 M \& E Workplan Development and Elements}

Monitoring \& Evaluation Workplan $=$ is a flexible guide to the steps used to document project activities, show progress towards project goals/objectives and answer evaluation questions. As a guide, the M\&E Workplan must include the following elements:

✓ goals/objectives of the overall plan as well as the evaluation questions and indicators,
✓ methodologies,
✓ implementation plan,
✓ matrix of expected results,
✓ proposed timeline and M\&E instruments for data collection.
To ensure that M\&E activities produce useful results, it is essential to (i) incorporate M\&E in the program design stage (ii) collaborate with all stakeholders to develop an integrated and comprehensive M\&E plan. M\&E workplans are often written to cover a four-five-year period as they may involve numerous M\&E efforts on multiple interventions for different target populations and both short and long-term goals/objectives.
In summary the key elements of a M\&E Workplan are: 1) The scope of the M\&E specifying goals and developing conceptual framework that integrates the inputs, activities, outputs, outcomes and impacts and establishes realistic expectations for what M\&E can produce; 2) The methodological approach - determining M\&E methods including identification of outcome indicators, data source, and plans for data analysis; 3) The implementation plan - delineating activities, roles, responsibilities and a timetable for identified activities with realistic expectations of when data will be analyzed and results will be available and 4) A plan for disseminating and using the results: - determining who will translate the results into terms understandable to program designers, managers and decision-makers, stakeholders' feedback sessions and the implications for future monitoring.

\subsection*{1.4.4 Evaluation Plan and Team}

\section*{- Rationale for developing an evaluation plan}

It is often helpful in the beginning stages to review with all stakeholders the reasons for developing a comprehensive evaluation plan. Some of the benefits that can be derived from the evaluation planning process are:

✓ Evaluation planning will provide program managers and stakeholders alike with the opportunity to assess the evaluation needs, resources, capabilities, and priorities in their area.
✓ Having an evaluation plan will show stakeholders how the program plans to be accountable for the resources they have received.
✓ \In the process of developing the evaluation plan, existing data sources and past or concurrent evaluation activities are often identified. Capitalizing on such existing data sources and past evaluative efforts can lead to a more efficient, less redundant plan for new evaluation monies.

✓ Having a long-term evaluation plan can clarify future decision-making regarding evaluation priorities.
✓ Finally, having a comprehensive evaluation plan in place may also favorably influence donor decision-making.
- KEY ELEMENTS OF AN EVALUATION PLAN

In developing an evaluation plan, a planning team should focus on the following key elements:

✓ Scope of the evaluation-Specifying the goals and objectives of the program and developing a conceptual framework (or logic model) that integrates and correlates the inputs, activities, outputs, outcomes, and impact, and establishing realistic expectations for what the evaluation will provide or show.
✓ Methodological approach- developing an evaluation design, includes specifying the outcome indicators or measures and the data source and plans for data analysis.
✓ Implementation plan-Delineating activities, roles, and responsibilities, and a timetable for identified activities with realistic expectations of when data will be analyzed and results will be available.
✓ Dissemination plan for the results- Determining who will translate the results into language that is useful to program designers, managers, and decision makers, how findings will be disseminated (for example, written papers, oral presentations, program materials), and implications for setting priorities for future evaluation activities.

\section*{PRINCIPAL COMPONENTS OF PROGRAM MONITORING AND EVALUATION}

Principal components of M\& E include cost-effectiveness, efficiency , accountability and relevance analysis as described below:
- Cost-effectiveness Analysis

Cost-effectiveness analysis also measures program effectiveness, but expands the analysis by adding a measure of program cost per unit of effect (for example, per number of HIV infections averted). By comparing the costs and consequences of various interventions, cost analyses and cost effectiveness estimates can assist in priority setting, resource allocation decisions, and program design.

\section*{Efficiency}

Measures programme proper use of resources as per budget line items.
- Accountability

Measures how the programme is meeting the needs of the beneficiaries or communities based on the situation analysis which informed the development of the programme.
- Relevance

Assess whether the programme objectives are in line with the national and global policies.

\section*{Routine Monitoring and Performance Reviews for government projects}

Routine monitoring is the continuous tracking and reporting of priority information about a program or project, its inputs and intended outputs, outcomes and impacts while performance review is the degree to which an intervention or organization operates according to specific criteria or standards or guidelines or achieves results in accordance with stated goals or plans.
In Kenya, planning and budgeting is guided by the Medium Term Expenditure Framework (MTEF), a three-year rolling revenue and expenditure budget plan for the government both at the national and the county levels. The concept of the "three year rolling" timeline consists of the following:
- The current budget year (X): the budget for the current financial year.
- The next budget year $(\mathrm{X}+1)$ : the target period of the current budget process i.e. the year the current budget process is being prepared for.
- The following two outer years $(\mathrm{X}+2)$ and $(\mathrm{X}+3)$ : these are estimates of the likely expenditure to provide services beyond the next budget year.

Procedurally, determination of the health sector's priorities for the next subsequent financial year (FY) (x+1) should be dependent on a systematic review of the previous year ( $\mathrm{x}-1$ ) performance/achievement. However, this cycle is always broken and does not conform to the MTEF cycle. The reviews aid programming of interventions, promote use of evidence to determine priority setting and enhance accountability in resource allocation.
Organizations seeking to influence the budget and planning cycles of the Ministry of Health will need to acquaint themselves with this critical calendar, but most importantly, preparation is needed in the following areas:
- Structured and convincing arguments to influence the logic for planning and budget allocation
- Adequate knowledge of the datelines for making input
- Critical voices needed to facilitate the performance review and planning

The figure below demonstrates the linkages and key steps between the MTEF calendar, the performance review and the planning cycle.

\section*{Health sector performance review, planning and implementation cycle (Mid-Term Expenditure Framework Cycle -MTEF)}
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-26.jpg?height=1132&width=1509&top_left_y=151&top_left_x=308)

\section*{Key milestones in the MTEF and performance review cycle}
I. The approved budgets for the current $\mathrm{FY}(\mathrm{X})$ for both national and county governments are launched in July of every financial year. The launch allows the start of the implementation of the activities of both governments through a work plan, as entrenched in the Constitution (no spending without a plan).
II. As the entities continue implementing year (X), a performance review process should be initiated from August of the same year (X). The review is for the year ( $\mathrm{X}-1$ ) and informs the year ( $\mathrm{X}+1$ ). The performance reviews is aimed at identifying achievements against the targets for year (X-1), factors that impact on the achievements and identify priorities, and interventions and projects for the year ( $\mathrm{X}+1$ ). This process should be steered by an already constituted M\&E governance structure at both the national and county levels. At the county-level, performance reviews start with the lowest planning units at the subcounty level, including community units, health facilities and the subcounty health management teams.
III. In the month of September every year, counties, through their county departments of health, consolidate the sub counties' reports into county- specific health review reports; this should be followed by a mini summit for stakeholders at the county level to agree on the priorities.
IV. In the month of October every year, a consolidation of performance review reports from planning entities at the national and county levels is done to form an annual health sector performance review report. This activity should be steered by the M\&E unit at the national level and should precede the Annual Health Congress or Annual Health Summit in November. To ensure
completeness of the report, continuous engagement with stakeholders is required until the final report is signed off.
V. The Annual Health Sector Summit is held in November every year. The summit draws members from different stakeholders and is purposely meant to authenticate the review report plus the priorities identified for the year ( $\mathrm{X}+1$ ). Financing schemes of the already agreed priorities for the year ( $\mathrm{X}+1$ ) are discussed and gaps are identified. The process ties well with the release of the budget outlook papers for year ( $\mathrm{X}+1$ ) and allows sectors to fit the priorities as per the ceiling. At all levels that is, national and county summit forum, communiqués on the commitments should be drafted and signed by all stakeholders.
VI. Sectors (planning entities) should request development partners to consider funding programs that are a priority but which are not funded by the national treasury.

\section*{Criteria for program and activities prioritization}

Determination and selection of the key programs, projects and activities at all levels should adhere to the principles of equity, social accountability, efficiency, participation, people- centeredness and should further be guided by the following:
- Linkage of the program with objectives of Vision 2030 and MTPs
- Linkage to government flagship projects and interventions
- Degree to which the program addresses core poverty interventions
- Degree to which the program addresses the core mandate and expected outputs and outcomes of the program
- Linkage of the program with other programs
- Cost-effectiveness and sustainability of the program

I Response to the requirements and furtherance of the implementation of the Constitution
Emerging and re-emerging health issues (e.g. disaster management, disease surveillance, outbreak investigations)
- Continuity of the already ongoing projects and programs

The programs identified should follow the program-based budgeting (PBB) approach adopted by the Kenyan government since FY 2014/15 with clear program goals, objectives, outcomes, outputs, and inputs comprehensively costed.

\section*{FINANCIAL ACCOUNTING AND MANAGEMENT OF MONITORING AND EVALUATION}

Budgeting is the process of creating a plan to spend your money. This spending plan is called a budget. Creating this spending plan allows you to determine in advance whether you will have enough money to do the things you need to do or would like to do.
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-28.jpg?height=402&width=966&top_left_y=233&top_left_x=395)

\section*{Budgeting is simply balancing your expenses with your income. If they don't balance and you spend more than you make, you will have a problem. Many people don't realize that they spend more than they earn and slowly sink deeper into debt every year.}

\subsection*{1.1 Why is Budgeting so Important?}

Since budgeting allows you to create a spending plan for your money, it ensures that you will always have enough money for the things you need and the things that are important to you. Following a budget or spending plan will also keep you out of debt or help you work your way out of debt if you are currently in debt.

\section*{Introduction to Project Management}

Project management is the application of processes, methods, skills, knowledge and experience to achieve specific project objectives according to the project acceptance criteria within agreed parameters. Project management has final deliverables that are constrained to a finite timescale and budget.
A key factor that distinguishes project management from just 'management' is that it has this final deliverable and a finite timespan, unlike management which is an ongoing process. Because of this a project professional needs a wide range of skills; often technical skills, and certainly people management skills and good business awareness.

\section*{What is a project?}

A project is a unique, transient endeavour, undertaken to achieve planned objectives, which could be defined in terms of outputs, outcomes or benefits. A project is usually deemed to be a success if it achieves the objectives according to their acceptance criteria, within an agreed timescale and budget. Time, cost and quality are the building blocks of every project.
Time: scheduling is a collection of techniques used to develop and present schedules that show when work will be performed.
Cost: how are necessary funds acquired and finances managed?
Quality: how will fitness for purpose of the deliverables and management processes be assured?

\section*{The core components of project management are:}
- defining the reason why a project is necessary;
- capturing project requirements, specifying quality of the deliverables, estimating resources and timescales;
- preparing a business case to justify the investment;
- securing corporate agreement and funding;
- developing and implementing a management plan for the project;

Downloaded by Patrick Ngobiro (pngobiro@gmail.com)
- leading and motivating the project delivery team;
- managing the risks, issues and changes on the project;
- monitoring progress against plan;
- managing the project budget;
- maintaining communications with stakeholders and the project organisation;
- provider management;
- Closing the project in a controlled fashion when appropriate.

\section*{PROJECT BUDGET PLANNING}

Every project boils down to money. If you had a bigger budget, you could probably get more people to do your project more quickly and deliver more. That's why no project plan is complete until you come up with a budget. But no matter whether your project is big or small, and no matter how many resources and activities are in it, the process for figuring out the bottom line is always the same.
It is important to come up with detailed estimates for all the project costs. Once this is compiled, you add up the cost estimates into a budget plan. It is now possible to track the project according to that budget while the work is ongoing.
Often, when you come into a project, there is already an expectation of how much it will cost or how much time it will take. When you make an estimate early in the project without knowing much about it, that estimate is called a rough order-of-magnitude estimate (or a ballpark estimate). This estimate will become more refined as time goes on and you learn more about the project. Here are some tools and techniques for estimating cost:
- Determination of resource cost rates: People who will be working on the project all work at a specific rate. Any materials you use to build the project (e.g., wood or wiring) will be charged at a rate too. Determining resource costs means figuring out what the rate for labour and materials will be.
- Vendor bid analysis: Sometimes you will need to work with an external contractor to get your project done. You might even have more than one contractor bid on the job. This tool is about evaluating those bids and choosing the one you will accept.
- Reserve analysis: You need to set aside some money for cost overruns. If you know that your project has a risk of something expensive happening, it is better to have some cash available to deal with it. Reserve analysis means putting some cash away in case of overruns.
- Cost of quality: You will need to figure the cost of all your quality-related activities into the overall budget. Since it's cheaper to find bugs earlier in the project than later, there are always quality costs associated with everything your project produces. Cost of quality is just a way of tracking the cost of those activities. It is the amount of money it takes to do the project right.

\section*{Cost Management explained in 4 steps}
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-30.jpg?height=1098&width=1389&top_left_y=233&top_left_x=308)

Cost management is concerned with the process of finding the right project and carrying out the project the right way. It includes activities such as planning, estimating, budgeting, financing, funding, managing, controlling, and benchmarking costs so that the project can be completed within time and the approved budget and the project performance could be improved in time.
Cost management covers the full life cycle of a project from the initial planning phase towards measuring the actual cost performance and project completion. This article will explain the different steps and processes in Project Cost Management, in line with methods such as the Total Cost Management Framework of AACE International [1].

\section*{Step 1: Resource planning}

Resource planning is the process of ascertaining future resource requirements for an organization or a scope of work. This involves the evaluation and planning of the use of the physical, human, financial, and informational resources required to complete work activities and their tasks. Most activities involve using people to perform work. Some activities involve materials and consumables. Other tasks involve creating an asset using mainly information inputs (e.g., engineering or software design). Usually, people use tools such as equipment to help them. In some cases, automated tools may perform the work with little or no human effort.
Resource planning begins in the scope and execution plan development process during which the work breakdown structure, organizational breakdown structure (OBS), work packages, and execution strategy are developed. The OBS establishes categories of labor resources or responsibilities; this categorization facilitates resource planning because all resources are someone's responsibility as reflected in the OBS.

Resource estimating (usually a part of cost estimating) determines the activity's resource quantities needed (hours, tools, materials, etc.) while schedule planning and development determines the work activities be performed. Resource planning then takes the estimated resource quantities, evaluates resource availability and limitations considering project circumstances, and then optimizes how the available resources (which are often limited) will be used in the activities over time. The optimization is performed in an iterative manner using the duration estimating and resource allocation steps of the schedule planning and development process.

\section*{Step 2: Cost estimating}

Cost estimating is the predictive process used to quantify, cost, and price the resources required by the scope of an investment option, activity, or project. It involves the application of techniques that convert quantified technical and programmatic information about an asset or project into finance and resource information. The outputs of estimating are used primarily as inputs for business planning, cost analysis, and decisions or for project cost and schedule control processes.
The cost estimating process is generally applied during each phase of the asset or project life cycle as the asset or project scope is defined, modified, and refined. As the level of scope definition increases, the estimating methods used become more definitive and produce estimates with increasingly narrow probabilistic cost distributions.
The estimation of the time duration of activities must be considered concurrently with costs because costs are often dependent on time duration and resource requirements identified in cost estimating may affect the schedule. Iterative approaches are used because outcomes of a cost estimate often lead to changes in scope or plans. In fact, the estimating process can be viewed as part of the scope definition process because iterative trading off between cost and scope intertwine the processes.
With Cleopatra Cost Estimating, you can achieve
- Creation of all kinds of cost estimates, from factor to detailed estimate.
- Successful tendering as it allows you to estimate costs, request bids, analyze those bids and to keep track of its costs.
- Connected cost estimating and cost control. With the Cost Control Module, you can track project costs during the execution phase.
- Traditionally, management would ask to deliver an estimate based on the time remaining to the TA execution. More organizations now require estimators to follow a staged approach to estimating, delivering three different types of estimates during the preparation phase.

\section*{Step 3: Cost budgeting}

Budgeting is a sub-process within estimating used for allocating the estimated cost of resources into cost accounts against which cost performance will be measured and assessed. This forms the baseline for cost control. Cost accounts used from the chart of accounts must also support the cost accounting process. Budgets are often time-phased in accordance with the schedule or to address budget and cash flow constraints.

\section*{Step 4: Cost control}

Cost control is concerned with measuring variances from the cost baseline and taking effective corrective action to achieve minimum costs. Procedures are applied to monitor expenditures and performance against the progress of a project. All changes to the cost baseline need to be recorded and the expected final total costs are continuously forecasted. When actual cost information becomes available an important part of cost
control is to explain what is causing the variance from the cost baseline. Based on this analysis corrective action might be required to avoid cost overruns.
Below figure is a process map for project performance measurement [1]. This process should be run in a continuous improvement cycle until project completion:

\section*{Bonus Step: Benchmarking}

As a bonus step, it is wise to add Benchmarking to the project cost management process.
Benchmarking helps close the loop between project A and project B. The knowledge from project A (referring to the running and executed projects) are analyzed and the feedback is reflected in project B (the next projects). That's how an improvement cycle is created to increase project performance. Benchmarking is widely used by technical industries to improve the performance of the projects. The goal of project benchmarking is to store data from executed and running projects to extract valuable project metrics and to benchmark current estimates. Performing statistical analysis on historical data can result in valuable information on relationships between variables, which can be used to set up a reliable cost knowledgebase or calibrate existing ones.
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-32.jpg?height=673&width=816&top_left_y=1027&top_left_x=393)

Project Benchmarking as a bonus stage
It is important to note that project benchmarking does not only include the comparison between projects, as it is also interesting to compare revisions within a project.

\section*{PROJECT MANAGEMENT INFORMATION SYSTEM (PMIS)}

A project management information system (PMIS) is the coherent organization of the information required for an organization to execute projects successfully. A PMIS is typically one or more software applications and a methodical process for collecting and using project information. These electronic systems "help [to] plan, execute, and close project management goals." ${ }^{[1]}$ PMIS systems differ in scope, design and features depending upon an organisation's operational requirements.
The project management information system, which is part of the environmental factors, provides access to tools, such as a scheduling tool, a work authorization system, a configuration management system, an information collection and distribution system, or
interfaces to other online automated systems. Automated gathering and reporting on key performance indicators (KPI) can be part of this system.
Project Management Information System (PMIS) are system tools and techniques used in project management to deliver information. Project managers use the techniques and tools to collect, combine and distribute information through electronic and manual means. Project Management Information System (PMIS) is used by upper and lower management to communicate with each other.
Project Management Information System (PMIS) help plan, execute and close project management goals. During the planning process, project managers use PMIS for budget framework such as estimating costs. The Project Management Information System is also used to create a specific schedule and define the scope baseline. At the execution of the project management goals, the project management team collects information into one database. The PMIS is used to compare the baseline with the actual accomplishment of each activity, manage materials, collect financial data, and keep a record for reporting purposes. During the close of the project, the Project Management Information System is used to review the goals to check if the tasks were accomplished. Then, it is used to create a final report of the project close.
To conclude, the project management information system (PMIS) is used to plan schedules, budget and execute work to be accomplished in project management.

\section*{PROJECT MANAGEMENT INFORMATION SYSTEM SOFTWARE}

At the center of any modern PMIS is software. Project management information system can vary from something as simple as a File system containing Microsoft Excel documents, to a full blown enterprise PMIS software.

\section*{REPORT WRITING AND PRESENTATION}
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-34.jpg?height=1048&width=1584&top_left_y=151&top_left_x=306)

\section*{Report Writing}

Report writing is the creation of a structured document that precisely describes, and examines an event or occurrence. A report is a document that is short, sharp and specially written for a particular audience and purpose. This article looks into the various features of a report, reasons for writing a report, common mistakes to avoid when writing a report and stages involved in writing a compelling report.

\section*{Features of a report}

Being that reports provide factual information based on decisions that were made, various guidelines are followed to ensure that a report has the essentials of an effective report. Here are the features of a satisfying report:
- Has an abstract or summary that provides a brief synopsis of the contents.
- Has a specific purpose and target audience.
- Has clearly labeled sections and headings.
- May contain data presented in for example graphs or tables.
- Often the text is broken up-bullet points, lists: is not always continuous prose.
- Written concisely and to the point.
- Written formally and objectively.

\section*{Why write a report?}

There are various reasons behind writing a report. It may be assignments at school, work or personal reasons. They include:
- To communicate a research process clearly and in a simple manner.
- To give a logical structure to the methods, results, and findings of research.
- To inform the target audience of the findings and viable recommendations of the research.
- To provide a document that would be easy to read and navigate for the reader.

\section*{Common problems with reports}

When writing a report, there are various mistakes during the writing process that you should be careful not to make. A good report should be free of:
- A weak or confusing structure of your work.
- Use of inappropriate writing style.
- Poor grammar and punctuation.
- Incorrect or inadequate referencing.
- Use of irrelevant information.
- Unnecessary use of jargon.
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-35.jpg?height=971&width=1578&top_left_y=531&top_left_x=306)

\section*{Five ways to make your report more effective}

A simple arrangement should be followed to ensure you write an effective report. Below are five useful stages of report writing:
1. Read the brief or terms of reference carefully; maintaining the focus on the brief will help you keep the content of your report relevant throughout.
2. Plan; planning each section should be considered before you begin writing your work.
3. Relate your findings to the background research conducted which will help you place the results in a broader context.
4. Put yourself in the position of the reader by asking yourself if what you have written will make sense to someone else.
5. Edit and proofread your work thoroughly.

Whatever the type of report you are writing, it should be clearly structured and well written. An organized report saves your audience time as they can navigate through it with ease.

\section*{STEPS IN REPORT WRITING}

There are many different types of reports, including business, scientific and research reports, but the basic steps for writing them are the same. These are outlined below.
- Step 1: Decide on the 'Terms of reference'
- Step 2: Decide on the procedure
- Step 3: Find the information
- Step 4: Decide on the structure
- Step 5: Draft the first part of your report
- Step 6: Analyze your findings and draw conclusions
- Step 7: Make recommendations
- Step 8: Draft the executive summary and table of contents
- Step 9: Compile a reference list
- Step 10: Revise your draft report

\section*{Step 1: Decide on the 'Terms of reference'}

To decide on the terms of reference for your report, read your instructions and any other information you've been given about the report, and think about the purpose of the report:
- What is it about?
- What exactly is needed?
- Why is it needed?
- When do I need to do it?
- Who is it for, or who is it aimed at?

This will help you draft your Terms of reference.

\section*{Step 2: Decide on the procedure}

This means planning your investigation or research, and how you'll write the report. Ask yourself:
- What information do I need?
- Do I need to do any background reading?
- What articles or documents do I need?
- Do I need to contact the library for assistance?
- Do I need to interview or observe people?
- Do I have to record data?
- How will I go about this?

Answering these questions will help you draft the procedure section of your report, which outlines the steps you've taken to carry out the investigation.

\section*{Step 3: Find the information}

The next step is to find the information you need for your report. To do this you may need to read written material, observe people or activities, and/or talk to people.
Make sure the information you find is relevant and appropriate. Check the assessment requirements and guidelines and the marking schedule to make sure you're on the right track. If you're not sure how the marks will be assigned contact your lecturer.
What you find out will form the basis, or main body, of your report - the findings.

\section*{Step 4: Decide on the structure}

Reports generally have a similar structure, but some details may differ. How they differ usually depends on:
- The type of report - if it is a research report, laboratory report, business report, investigative report, etc.
- How formal the report has to be.
- The length of the report.

Depending on the type of report, the structure can include:
- A title page.
- Executive summary.
- Contents.
- An introduction.
- Terms of reference.
- Procedure.
- Findings.
- Conclusions.
- Recommendations.
- References/Bibliography.
- Appendices.
- The sections, of a report usually have headings and subheadings, which are usually numbered

\section*{Step 5: Draft the first part of your report}

Once you have your structure, write down the headings and start to fill these in with the information you have gathered so far. By now you should be able to draft the terms of reference, procedure and findings, and start to work out what will go in the report's appendix.

\section*{Findings}

The findings are result of your reading, observations, interviews and investigation. They form the basis of your report. Depending on the type of report you are writing, you may also wish to include photos, tables or graphs to make your report more readable and/or easier to follow.
Appendices
As you are writing your draft decide what information will go in the appendix. These are used for information that:
- is too long to include in the body of the report, or
- supplements or complements the information in the report. For example, brochures, spreadsheets or large tables.

\section*{Step 6: Analyze your findings and draw conclusions}

The conclusion is where you analyze your findings and interpret what you have found. To do this, read through your findings and ask yourself:
- What have I found?
- What's significant or important about my findings?
- What do my findings suggest?

For example, your conclusion may describe how the information you collected explains why the situation occurred, what this means for the organisation, and what will happen if the situation continues (or doesn't continue).
Don't include any new information in the conclusion.

\section*{Step 7: Make recommendations}

Recommendations are what you think the solution to the problem is and/or what you think should happen next. To help you decide what to recommend:
- Reread your findings and conclusions.
- Think about what you want the person who asked for the report should to do or not do; what actions should they carry out?
- Check that your recommendations are practical and are based logically on your conclusions.
- Ensure you include enough detail for the reader to know what needs to be done and who should do it.
Your recommendations should be written as a numbered list, and ordered from most to least important.

\section*{Step 8: Draft the executive summary and table of contents}

Some reports require an executive summary and/or list of contents. Even though these two sections come near the beginning of the report you won't be able to do them until you have finished it, and have your structure and recommendations finalised.
An executive summary is usually about 100 words long. It tells the readers what the report is about, and summarise the recommendations.

\section*{Step 9: Compile a reference list}

This is a list of all the sources you've referred to in the report and uses APA referencing.

\section*{Step 10: Revise your draft report}

It is always important to revise your work. Things you need to check include:
- If you have done what you were asked to do. Check the assignment question, the instructions/guidelines and the marking schedule to make sure.
- That the required sections are included, and are in the correct order.
- That your information is accurate, with no gaps.
- If your argument is logical. Does the information you present support your conclusions and recommendations?
- That all terms, symbols and abbreviations used have been explained.
- That any diagrams, tables, graphs and illustrations are numbered and labelled.
- That the formatting is correct, including your numbering, headings, are consistent throughout the report.
- That the report reads well, and your writing is as clear and effective as possible.
![](https://cdn.mathpix.com/cropped/2025_11_19_d8a5a88a857679200a38g-38.jpg?height=730&width=1207&top_left_y=1203&top_left_x=460)

\section*{Report Writing Format}

Here are the main sections of the standard report writing format:
- Title Section - This includes the name of the author(s) and the date of report preparation.
- Summary - There needs to be a summary of the major points, conclusions, and recommendations. It needs to be short as it is a general overview of the report. Some people will read the summary and only skim the report, so make sure you include all the relevant information. It would be best to write this last so you will include everything, even the points that might be added at the last minute.
- Introduction - The first page of the report needs to have an introduction. You will explain the problem and show the reader why the report is being made. You
need to give a definition of terms if you did not include these in the title section, and explain how the details of the report are arranged.
- Body - This is the main section of the report. There needs to be several sections, with each having a subtitle. Information is usually arranged in order of importance with the most important information coming first.
- Conclusion - This is where everything comes together. Keep this section free of jargon as most people will read the Summary and Conclusion.
- Recommendations - This is what needs to be done. In plain English, explain your recommendations, putting them in order of priority.
- Appendices - This includes information that the experts in the field will read. It has all the technical details that support your conclusions.
Remember that the information needs to be organized logically with the most important information coming first.
--- Converted MMD End ---
