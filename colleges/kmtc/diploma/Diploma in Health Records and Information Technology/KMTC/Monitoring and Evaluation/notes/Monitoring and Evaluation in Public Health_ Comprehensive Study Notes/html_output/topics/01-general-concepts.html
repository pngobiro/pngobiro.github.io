<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Topic 1: General M&E Concepts</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles/main.css">
    <!-- Include MathJax -->
    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams' // Use AMS numbering of equations
        },
        svg: { fontCache: 'global' },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <article class="document-section" role="main">
            <!-- Top Navigation -->
            <nav class="document-nav" role="navigation" aria-label="Document Sections">
                <div class="nav-links">
                    <a href="../index.html" class="nav-button">
                        <span class="nav-icon"></span>
                        <span class="nav-text">Table of Contents</span>
                    </a>
                    <a href="#" class="nav-button disabled">
                        <span class="nav-icon">←</span>
                        <span class="nav-text">Previous Section</span>
                    </a>
                    <div class="document-progress">
                         <div class="progress-bar" aria-hidden="true">
                            <div class="progress-fill" style="width: 25%;"></div>
                        </div>
                       <span class="progress-text">Topic 1 of 4</span>
                    </div>
                    <a href="02-system-and-framework.html" class="nav-button">
                        <span class="nav-text">Next Section</span>
                        <span class="nav-icon">→</span>
                    </a>
                </div>
            </nav>

            <header class="section-header">
                <h1 class="section-title">Monitoring and Evaluation Notes</h1>
                <div class="title-underline"></div>
            </header>

            <main class="section-content">
                <!-- START: intro -->
                <section id="intro" class="content-section">
                    <h2 class="section-heading">Health records information technology (Kenya Medical Training College)</h2>
                    <div class="content-card">
                        <p>Scan to open on Studocu</p>
                        <h3>MONITORING AND EVALUATION</h3>
                        <p>By the end of this module, the learner should;</p>
                        <ol class="enhanced-list">
                            <li>Develop an understanding of the project management</li>
                            <li>Establish an understanding of M&E concepts, methods and processes</li>
                            <li>Establish an understanding of qualitative and quantitative data in monitoring and evaluation</li>
                            <li>Formulate, design, monitoring and evaluation systems</li>
                            <li>Understand financial accounting and management in monitoring and evaluation</li>
                            <li>Develop and implement Performance Monitoring and Evaluation plans</li>
                            <li>Develop skills in report writing and presentation.</li>
                        </ol>
                    </div>
                </section>
                <!-- END: intro -->

                <!-- START: background -->
                <section id="background" class="content-section">
                    <h2 class="section-heading">Background Information</h2>
                    <div class="content-card">
                        <p>Monitoring and evaluation (M&E) has gained increasing significance in the health sector during the last decade, partly due to increasing public demand for measurement and accountability in the use of health sector resources. The Constitution of Kenya 2010 and attendant legislation have raised the public's expectations about the ability of national and county governments to put in place measures that increase transparency, accountability and public participation in the implementation of health programs. Both the Kenya Health Policy 2014-2030 and the Kenya Health Sector Strategic Plan 20142018 have integrated the requirements of a robust M&E system to ensure systematic tracking of investments and progress while promoting a culture of evidence- based planning and decision making. Kenya's Ministry of Health has set out to strengthen M&E systems within the health sector through a wide range of capacity development initiatives. The Health Sector M&E Framework 2014-2018 enables all actors to work within convergent efforts to achieve the targets set within the Kenya Health Sector Strategic Plan 2014-2018. Despite these positive milestones, several studies and assessments have documented capacity gaps in implementing a fully functional M&E system in the health sector.</p>
                        <p>Increased monitoring and evaluation (M&E) capacity is vital to ensure that health resources are used effectively. Indeed, functioning M\&E systems will play a major role in evidence-based programming and attracting future resources. In response to rapidly growing demand, the Department of Health Records and Information at Kenya Medical Training College has established a higher diploma program to provide M&E training and technical support services. This program offers an M&E course designed to equip Health Records and Information officers and overall health practitioners with the concepts and practical skills required to design and manage effective M&E systems in all health programs.</p>
                        <p>The course is intended for Higher diploma students in public health whose future employment may take them into the agencies and government organizations with responsibilities for M&E planning and implementation, project management, research work, information management and line management. Such agencies and organizations include:</p>
                        <ul class="enhanced-list">
                            <li>Consulting firms specializing in project management, monitoring and evaluation</li>
                            <li>National government, provincial and municipal health, epidemiology and HIV/AIDS departments</li>
                            <li>International and national NGOs</li>
                            <li>Multilateral and bilateral international development assistance agencies</li>
                        </ul>
                    </div>
                </section>
                <!-- END: background -->

                <!-- START: topic-1 -->
                <section id="topic-1" class="content-section">
                    <h2 class="section-heading">TOPIC 1: GENERAL M&E CONCEPTS</h2>
                    
                    <!-- START: 1.1 -->
                    <div class="content-card">
                        <h3>1.1 PROGRAM M&E RATIONALE</h3>
                        <p>Public health practitioners (PHP) need knowledge and advanced skills in order to provide various levels of accountability for their activities or policies to a variety of constituencies. It is becoming increasingly important for countries to be able to report accurate, timely and comparable data to national authorities and donors in order to secure continued funding for expanding health programmes and, most importantly, to utilize this information locally to strengthen evolving programmes. This course aims to ensure that PHPs are able to measure, report, and use good quality health and health-related information in a manner that meets both donor and country needs. It is particularly important for national programme implementers and managers to have access to the quality information they need to make adjustments and programmatic and technical decisions (Data for Decision Making).</p>
                        <p>As such, a common, comprehensive and coherent M&E system has several advantages: It contributes to more efficient use of data and resources by ensuring, for example, that indicators and sampling methodologies are comparable over time and by reducing duplication of effort. Where resources are scarce, this is an important asset. Data generated by a comprehensive M&E system ought to serve the needs of many constituents, including programme or project managers, researchers and donors, eliminating the need for each to repeat baseline surveys or evaluation studies when they might easily use existing data.</p>
                        <p>From the point of view of the national programme, a coherent M&E system helps ensure that donor-funded M&E efforts best contribute to national needs. These needs go beyond disease-focused M&E, rather than simply serving the reporting needs of specific international donors or organizations.</p>
                        <p>Agreement among the major donor, technical and implementing agencies on the basic core M&E framework will reduce the burden of requests for data from different agencies. Shared planning, execution, analysis or dissemination of data collection can reduce overlap in programming and increase cooperation between different groups, many of whom may work more efficiently together than in isolation..</p>
                    </div>
                    <!-- END: 1.1 -->

                    <!-- START: 1.2 -->
                    <div class="content-card">
                        <h3>1.2 CONCEPTUAL APPROACH AND FRAMEWORK FOR M&E</h3>
                        <p>Several considerations underlie the decision-making process about health program monitoring and evaluation. The selection of an appropriate M&E concept for example an AIDS prevention program is crucial because it determines the guiding philosophy behind the actual monitoring and evaluation process. A number of theorists and M&E practitioners have proposed various conceptual approaches to M&E. These approaches differ in their conception as to what M&E is, what the relationship with the primary client and other stakeholders should be, who should be making the relevant value judgments regarding the program, and the criteria for judging the evaluation process itself. The conceptual approach debate was, and is for the most part, a debate about the best ways to measure and interpret change. It has highlighted a series of methodological dimensions among which there are variations in emphasis. These dimensions focus attention on some of the options available for making decisions about methods. Today, for example, there is consensus that both quantitative and qualitative data are valued and recognized as legitimate for program M&E. In fact, these methods are by no means incompatible and should be used in combination. Deciding what and how much data to gather in M&E involves difficult methodological decisions and trade-offs between the quality and utility of information.</p>
                        <p>M&E approach that uses multiple data collection methods, both quantitative and qualitative, is more likely to address diverse evaluation needs than is a more limited approach. At the same time, research priorities must be sensitive to competing needs for resources in an environment in which for example HIV/AIDS epidemic is growing rapidly and evaluation is sometimes considered a luxury. It is a major task of the evaluator to match research methods to the reality of particular M&E questions and to the available resources. There is also a need for evaluation researchers to play an active role, not merely a consultative one, in making design decisions for program assessments. Although a program M&E should be a neutral scientific observer, he or she can also mediate between different stakeholder groups, can enable others through a participatory evaluation approach, and can advocate for the dissemination of evaluation results within the larger arena of decision making. Planning M&E and data collection activities in a participatory fashion is essential for achieving the delicate balance between practical needs and methodological desirability.</p>
                        <p>Key stakeholders should be included in the planning process and every effort should be made for effective use of limited resources. Ensuring active support and participation of key stakeholders who have an interest in the results obtained by various data collection systems is particularly important for programs funded by external donors that use host country institutions for data collection activities. Data produced by these efforts will have a better chance to be timely and of acceptable quality. Whenever possible, participants, including implementing institutions, host-country collaborators and local representatives of donor agencies, should attempt to reach consensus regarding data needs.</p>
                    </div>
                    <!-- END: 1.2 -->

                    <!-- START: 1.3 -->
                    <div class="content-card">
                        <h3>1.3 MONITORING AND EVALUATION</h3>
                        <p>The term monitoring and evaluation (M&E) quite broadly includes any effort to increase human effectiveness through systematic data-based inquiry. When one examines and judges accomplishments and effectiveness, one is engaged in monitoring and evaluation. When this examination of effectiveness is conducted systematically and empirically through careful data collection and thoughtful analysis, one is engaged in evaluation research. Thus evaluation is applied research or a type of "action science." This distinguishes evaluation research from basic academic research....The purpose of applied research and evaluation is to inform action, enhance decision-making, and apply knowledge to solve human and societal problems. Applied evaluative research is judged by its usefulness in making human actions and interventions more effective and by its practical utility to decision makers, policymakers and others who have a stake in efforts to improve the world." Such an evaluation approach is utilization focused. This approach emphasizes the interests of key stakeholders and primary users of the information at all levels, for example the donor, the host country, and the implementing agencies. It applies socio-epidemiological research to identify ways to improve the design and implementation of prevention and care programs. This topic therefore describes several considerations that are fundamental to planning an evaluation effort. It then presents a comprehensive framework for country programs by explaining the major types of evaluation and discussing several important issues related to planning evaluation programs and improving their ability to measure program effects.</p>
                        
                        <h4>1.3.1 Types of a programme or project M&E are:</h4>
                        <ol class="enhanced-list">
                            <li>Formative assessment and research. Through formative assessment, the program: Identify gaps, define realistic goals/objectives for interventions, and identifies intervention needs and strategies;</li>
                            <li>Monitoring: As a routine process of data collection and measurement of progress toward objects. Three main domains of information required in a monitoring system are inputs, processes, and outputs;</li>
                            <li>Evaluation: Assessing outcomes and impacts and are connected to process monitoring and</li>
                            <li>Cost-effectiveness analysis including sustainability issues.</li>
                        </ol>

                        <h4>1.3.2 A comprehensive M&E framework for country programs</h4>
                        <p>Prevention and care programs need to be monitored and evaluated at different phases of the program cycle. All stages of evaluation have to be considered together to provide an overall picture of the program because no single data collection approach can supply all the information necessary to improve program performance or affect policy change. Multiple complementary evaluation approaches and multiple methodologies (qualitative and quantitative) have to be applied to address different evaluation needs..</p>

                        <h5>- Formative Evaluation</h5>
                        <p>Formative evaluation should be conducted during the planning (or replanning) stage of prevention and care program to identify and resolve intervention and evaluation issues before the program is widely implemented. This is the time when flexibility is greatest and program sponsors are freer to make decisions about how to proceed. Formative evaluation explores the need for interventions, provides the information necessary to define realistic goals and objectives for the program interventions, and helps program planners make tentative decisions about effective, feasible intervention strategies and how to carry them out. Formative evaluation can also be used as an exploratory tool as the project is being carried out to provide feedback to project managers to help them adjust program objectives to changing situations.</p>
                        <p>Formative evaluation research can identify unacceptable or ineffective intervention approaches, designs, and concepts.. In many cases, interventions have been based on ideas developed outside of the context of the lives of the people to whom the interventions have been delivered. The literature on behavioral change interventions is full of examples of ideas that made perfect sense in the abstract but failed completely in the "real world," mainly because the ideas were unacceptable to the target audience or were not stated in ways that were relevant to the lives of those people. A fuller understanding of the issues might well have led planners to redesign the intervention to make it more appealing to the selected audience. Fortunately, this situation is changing because formative evaluation is now being applied more frequently in designing prevention programs. Formative evaluations use a mix of research methods that can rapidly provide relevant information to program designers. These methods include:</p>
                        <ul class="enhanced-list">
                            <li>Reviews of existing information;</li>
                            <li>Focus group discussions;</li>
                            <li>Individual in-depth interviews;</li>
                            <li>Participant observations; and</li>
                            <li>Short quantitative surveys with structured</li>
                            <li>Questionnaires</li>
                        </ul>
                        <p>The most frequently cited methodological criticism of formative evaluation is its lack of external validity or generalizability. Because the results of the evaluation derive from small-scale rapid assessment procedures and/or pilot studies, one cannot generalize from them to a larger population. Despite this limitation, formative evaluation research can usually identify unacceptable or ineffective intervention approaches, designs, and concepts. However, even with adequate formative evaluation at the program planning stage, there is no guarantee that a prevention program will be effective when finally implemented; it may not be implemented adequately enough to be effective.</p>

                        <h5>- Process Evaluation</h5>
                        <p>Once activities are underway, there is a need to examine whether they are being carried out correctly, on time, and within budget. Process evaluation addresses such basic questions as, "To what extent are planned intervention activities actually realized?" and "What services are provided, to whom, when, how often, for how long, and in what context?" Both input (the basic resources required in terms of manpower, money, material, and time) and output (the immediate service improvement expressed as distributed commodities, trained staff, and service units delivered) are key elements of process evaluation. These questions are often answered in quantitative terms.</p>
                        <p>Qualitative evidence of how and why a prevention program works or fails to work is equally important in answering process evaluation questions. Process evaluation requires getting close to data, becoming intimately acquainted with the details of the program, and observing not only anticipated effects but also unanticipated consequences. An understanding of the processes through which intervention activities achieve effects can help to explain the outcome of the intervention. Process evaluation, however, does not demonstrate whether interventions are effective. Process evaluation can also play an important role in improving or modifying interventions by providing the information necessary to adjust delivery strategies or program objectives in a changing epidemic. Process-oriented evaluation is carried out throughout the course of the program implementation and should use different methodological approaches to assess service delivery, ranging from reviews of service records and regular reporting systems, key informant interviews, exit interviews of service users, direct observations by 'mystery clients' (for example, in sexually transmitted infection [STI] and voluntary counseling and testing [VCT] services) to quantitative population-based surveys to assess program coverage and barriers to service use. Different qualitative and quantitative study designs that are complementary to one another provide together the most comprehensive information.</p>

                        <h5>- Effectiveness (Efficiency and Accountability) Evaluation: Assessing Outcome and Impact</h5>
                        <p>Evaluating the effectiveness of a disease prevention programs will almost always require quantitative measurements. These measurements will assess the extent to which the objectives of the program were achieved. Effectiveness evaluation is used to answer the questions, "What outcomes were observed?," "What do the outcomes mean?," and "Does the program make a difference?" Taking into account the various implementation stages of HIV/AIDS prevention programs and the fact that, over time, new age cohorts become sexually active, it is advisable to stratify effectiveness evaluation by short-term and intermediate program effects (program outcome) and long-term program effects (program impact).</p>
                        <p>Table below provides examples of program outcome and impact measures for these different stages.</p>

                        <div class="table-container">
                            <table class="content-table">
                                <thead>
                                    <tr>
                                        <th>Program Outcome (short-term and intermediate effects)</th>
                                        <th>Program Impact (long-term effects)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Changes in HIV/AIDS-related attitudes</td>
                                        <td>Sustained changes in HIV/STI-related risk behaviors</td>
                                    </tr>
                                    <tr>
                                        <td>HIV/STI-related risk behaviors</td>
                                        <td>Trends in HIV/AIDS rates</td>
                                    </tr>
                                    <tr>
                                        <td>Trends in STI rates (e.g., gonorrhea</td>
                                        <td>AIDS-related mortality rates</td>
                                    </tr>
                                    <tr>
                                        <td>Increase in social support/community response</td>
                                        <td>Reduced individual and societal vulnerability to HIV/AIDS</td>
                                    </tr>
                                    <tr>
                                        <td></td>
                                        <td>Sustained changes in societal norms</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <p>Changes in HIV/AIDS-related attitudes, the reduction of risk behaviors and adoption of protective behaviors, and changes in STI rates are considered to be the most appropriate short-term or intermediate (also called proximate) outcome measures for interventions designed to reduce sexual transmission of HIV. Long-term effects include impact on HIV/AIDS trends, sustainability issues, and improved societal response. Outcome and impact evaluation is intimately connected with process evaluation. Process information can help the evaluator to understand how and why interventions have achieved their effects and, perhaps, what is actually making the difference. Examining outcome/impact indicators without assessing the process of program implementation could lead to erroneous conclusions regarding the effectiveness of the intervention. Program goals and objectives have to be carefully defined to allow the selection of appropriate outcome and impact measures to assess the effectiveness of an AIDS prevention program. Effectiveness evaluation is generally based on indicators that provide quantitative value from which the outcome and impact of interventions can be measured. Because multiple interventions working synergistically together are most effective in producing behavior change, surveys should not be typically designed to capture the effects of one single intervention. Rather, they should be designed to measure behavioral trends in population groups who are exposed to combined interventions. The evaluation of one intervention is usually conducted through rigorous and expensive controlled trials.</p>
                        
                        <ul class="enhanced-list">
                            <li>Cost-effectiveness Analysis</li>
                        </ul>

                        <p>Evaluating the effectiveness of prevention programs will almost always require quantitative measurements. These measurements will assess the extent to which the objectives of the program were achieved. Effectiveness evaluation is used to answer the questions, "What outcomes were observed?," "What do the outcomes mean?," and "Does the program make a difference?" For example taking into account the various implementation stages of HIV/AIDS prevention programs and the fact that, over time, new age cohorts become sexually active, it is advisable to stratify effectiveness evaluation by short-term and intermediate program effects (program outcome) and long-term program effects (program impact).</p>
                        <p>Outcome and impact evaluation is intimately connected with process evaluation. Process information can help the evaluator to understand how and why interventions have achieved their effects and, perhaps, what is actually making the difference. Examining outcome/impact indicators without assessing the process of program implementation could lead to erroneous conclusions regarding the effectiveness of the intervention.</p>
                        <p>Program goals and objectives have to be carefully defined to allow the selection of appropriate outcome and impact measures to assess the effectiveness of an AIDS prevention program. Effectiveness evaluation is generally based on indicators that provide quantitative value from which the outcome and impact of interventions can be measured. Because multiple interventions working synergistically together are most effective in producing behavior change, surveys should not be typically designed to capture the effects of one single intervention .Rather, they should be designed to measure behavioral trends in population groups who are exposed to combined interventions. The evaluation of one intervention is usually conducted through rigorous and expensive controlled trials.</p>
                        <p>Cost-effectiveness analysis also measures program effectiveness, but expands the analysis by adding a measure of program cost per unit of effect. By comparing the costs and consequences of various interventions, cost analyses and cost effectiveness estimates can assist in priority setting, resource allocation decisions, and program design. The attribution dilemma: are observed changes a result of prevention interventions?</p>
                        <p>Program evaluation is intrinsically complex, however, due to the temporal evolution of epidemics and our poor understanding of how different behaviors and epidemiologic factors influence epidemic patterns as they move from an epidemic phase to an endemic state. Several factors unrelated to intervention effects can contribute to the observed stabilization or decreases in the prevalence or incidence of diseases in a given setting. They include:</p>
                        <ul class="enhanced-list">
                            <li>Mortality, especially in mature epidemics;</li>
                            <li>Saturation effects in populations at high risk;</li>
                            <li>Behavioral change in response to the experience of disease among friends and relatives;</li>
                            <li>Differential migration patterns related to the epidemic; and</li>
                            <li>Sampling bias and/or errors in data collection and analysis.</li>
                        </ul>
                        <p>Determining whether observed changes in disease incidence and prevalence are a reflection of the natural history of the epidemic or due to intervention effects is a critical evaluation issue. This is particularly true when evaluating behavior changes in the face of growing numbers of people with AIDS-related illnesses because there is evidence that secular trends toward risk reduction will occur. For example, having a friend or relative with HIV/AIDS may influence adolescents to delay the onset of sexual relations or motivate those with non-regular sex partners to use condoms. Their different perspectives on this issue also reflect fundamental differences regarding the criteria for judging the process of program evaluation itself. From a public health perspective, it may not matter whether the observed changes are due to a particular intervention. From the costeffectiveness or policy perspective, however, it is important to determine what caused the observed changes in sexual behavior. If the changes would have occurred without a particular intervention that was designed to contribute to the observed changes, the costs of the intervention could be considered as resources better spent on something more useful. Prevention programs are under growing pressure to estimate which approaches work best for specific target populations in different epidemiologic settings with a given level of inputs in order to allocate resources in a cost-effective manner.</p>
                        <p>Effectiveness evaluation, therefore, is critical because it can answer a basic question, "Does the program make a difference?" A vexing task of assessing program effectiveness is to disentangle the attributable affects of a prevention program from the gross outcome and impact observed. Such estimates can be made with varying degrees of plausibility, but not with certainty. A general principle applies here: The more rigorous the research design, the more convincing the resulting estimate. A hierarchy of evidence based on the study design can be established that reflects the degree of certainty in concluding that a given proportion of the observed changes in behavior is attributable to the intervention program and is not the result of other factors.</p>
                        <p>The interpretation of program evaluation data should always be approached with caution. In most situations, the program and evaluation process as a whole is not a rigorously controlled experimental trial. The ability of an evaluation to precisely determine the true extent of a program's effectiveness is often limited by time, resources, and the lack of a rigorous design. Many factors can confuse or confound the results measured, and biases can be introduced by a range of factors inherent to the problem of HIV/AIDS, the available measurement options, and those conducting the evaluation. One of the most difficult questions to answer in any evaluation is that of attributing any measured effect to the program being evaluated. Defining the web of interacting and overlapping influences is extremely difficult, and is one of the reasons why so many programs have difficulty attributing results to their actions. At some point, we need to stop worrying about attribution in such settings and focus on monitoring the changes as they occur.</p>

                        <h4>1.3.3 What is the difference between national and sub-national M&E?</h4>
                        <p>In view of scarce M&E resources at sub-national level, emphasis is placed on monitoring programme inputs and outputs and assessing whether or not implementation progresses according to a sub-national plan. A small facility assessment as part of a routine supervision could serve to provide information on the quality of care or the availability and utilization of services. At all levels, both monitoring and evaluation are needed. Subnational data is extremely relevant for national level M&E provided that national guidelines are followed to make aggregation possible. Information gathered from the subnational level is helpful in guiding policy discussions and in validating results at higher levels. In some cases, data from the sub-national provides a better indication of trends. For example, if a country has actual data on condom distribution by district (or equivalent) instead of one national overall figure, monitoring of trends in condom use may become more meaningful and more accurate.</p>

                        <h4>1.3.4 What is the difference between programme and project M&E?</h4>
                        <p>Programme refers to an overarching national or sub-national response to the disease. Within a national programme, there are typically a number of different areas of programming. For example, the HIV/AIDS programme has a number of "subprogrammes or projects" such as blood safety, STI control, or HIV prevention for young people.</p>
                        <p>Project refers to a mix of interventions with activities supported by resources that aim at a specific population defined geographically or otherwise. It should be noted that projects and programmes can also be defined by timeframes - projects are usually short term where as programmes are usually longer term in scope. In view of its wider scope (thematic, geographic, target population), programme monitoring tends to be more complex than project monitoring and requires strong coordination among all implementing agencies.</p>
                        <p>Programme evaluation is even more difficult, especially for certain types of evaluations (outcome and impact evaluations). For such evaluations to be conducted, the design of the programme/project must include its own baseline and follow-up assessments measuring not only specific outcomes but also the level of exposure to the programme/project and its activities. (See question 4 for more details on evaluations)</p>

                        <h4>1.3.5 Monitoring (M)</h4>
                        <p>Monitoring is the routine tracking of the key elements of programme/project performance, usually inputs and outputs, through record-keeping, regular reporting and surveillance systems as well as health facility observation and client surveys. Monitoring helps programme or project managers determine which areas require greater effort and identify areas that might contribute to an improved response. In a well-designed monitoring and evaluation system, monitoring contributes greatly towards evaluation. Indicators selected for monitoring will be different depending on the reporting level within the health system. It is very important to select a limited number of indicators that will actually be used by programme implementers and managers. There is a tendency to collect information on many indicators and report this information to levels where it will not and cannot be used for effective decision-making.</p>

                        <h4>1.3.6 Evaluation (E)</h4>
                        <p>In contrast to monitoring, evaluation is the episodic assessment of the change in targeted results that can be attributed to the programme or project/project intervention. In other words, evaluation attempts to link a particular output or outcome directly to an intervention after a period of time has passed. Evaluation helps programme or project managers determine the value or worth of a specific programme or project. Costeffectiveness and cost-benefit evaluations are useful in determining the added value of a particular programme or project. Evaluation aims at assessing outcome and impact of a program.</p>
                        <p>The timing for a specific type of evaluation depends on the implementation status of a programme or project.</p>
                        <h5>There are four types of programme or project evaluations:</h5>
                        <ul class="enhanced-list">
                            <li>Formative evaluation</li>
                            <li>Process evaluation</li>
                            <li>Outcome evaluation</li>
                            <li>Impact evaluation</li>
                        </ul>
                        <p>Formative evaluation is conducted in the design phase of prevention and care programme to identify and resolve intervention and evaluation issues before the programme is widely implemented. Formative evaluation identifies transmission dynamics assists in identifying effective interventions and helps define realistic goals.</p>
                        <p>Process evaluation involves the assessment of the programme or project's content, scope or coverage together with the quality of implementation. If the process evaluation finds that the programme/project has not been implemented, or is not reaching its intended audience, it is not worth conducting an outcome evaluation. However, if process evaluation shows progress in implementing the programme/project as planned, then it is worth carrying out such an evaluation.</p>
                        <p>Outcome evaluation is designed specifically with the intention of being able to attribute the changes to the intervention itself. At the very least, the evaluation design has to be able to plausibly link observed outcomes to a well-defined programme or project, and to demonstrate that changes are not the result of non-programme/project factors.</p>
                        <p>Impact evaluation: If the evaluation shows a change in outcomes, then it is time for impact evaluation. True impact evaluation, able to attribute long-term changes to a specific programme or project, is very rare and quite costly. Rather, monitoring impact indicators taken in conjunction with process and outcome evaluations are considered to be sufficient to indicate the overall impact.</p>
                        <p>As seen above, the objectives and the methodology used in monitoring and evaluation are different. In general, evaluations are more difficult in view of the methodological rigor needed; without such rigor, wrong conclusions on Monitoring and Evaluation toolkit the value of a programme or project can be drawn. They are also more costly, especially outcome and impact evaluations which require population-based surveys.</p>

                        <h4>1.3.7 M&E Indicators</h4>
                        <p>Indicators are markers designed to monitor and evaluate program performance/ effectiveness or goals/objectives. As illustrated below, types are input, output, outcome and impact indicators based on life cycle of a program. Ideal indicator should have the following seven characteristics:</p>
                        <ul class="enhanced-list">
                            <li>Valid-They should measure the condition or event they are intended to measure.</li>
                            <li>Reliable-They should produce the same results when used more than once to measure the same condition or event.</li>
                            <li>Specific-They should measure only the condition or event they are intended to measure.</li>
                            <li>Sensitive-They should reflect changes in the state of the condition or event under observation.</li>
                            <li>Operational-It should be possible to measure or quantify them with developed and tested definitions and reference standards.</li>
                            <li>Affordable-The costs of measuring the indicators also should be reasonable.</li>
                            <li>Feasible-It should be possible to carry out the proposed data collection.</li>
                        </ul>
                        <p>Validity is inherent in the actual content of the indicator and also depends on its potential for being measured. Reliability is inherent in the methodology used to measure the indicator and in the person using the methodology. Interpreting outcome indicators for behavioral interventions that promote safer behaviours is further complicated by the fact that risk behaviors are measured in relative terms. For example, percentage figures of condom use measure the proportion of sexual exposures that are considered to be safe. These may or may not reflect the absolute number of sex acts that place individuals at risk for exposure to sexual transmission. Ten percent condom use in 10 HIV-associated sexual episodes is still "safer" than 75 percent condom use in 100 HIV-associated sex episodes ( 9 versus 25 unprotected HIV-associated sex acts, respectively). Therefore, in this example it also would be important to determine the frequency of condom use in absolute terms in a given risk situation. Behavioral surveys have begun to address this dilemma by collecting additional data on "always or consistent" condom use in the context of sexual episodes with non-regular partners.</p>
                        <p>Indicators are used at different levels to measure what goes into a programme or project and what comes out of it. Over the past few years, one largely agreed upon framework has emerged, the input-process-output-outcome-impact framework illustrated below.</p>
                        <div class="equation">$$ \text { Input ⟶ process } \text { ⟶ } \text { output } \text { ⟶ } \text { outcome } \text { ⟶ } \text { impact } $$</div>
                        <p>For a programme or project to achieve its goals, inputs such as money and staff time must result in outputs such as stocks and delivery systems for drugs and other essential commodities, new or improved services, trained staff, information materials, etc. These outputs are often the result of specific processes, such as training sessions for staff, that should be included as key activities aimed at achieving the outputs. If these outputs are well designed and reach the populations for which they were intended, the programme or project is likely to have positive short-term effects or outcomes, for example increased condom use with casual partners, increased use of insecticide-treated nets (ITNs), adherence to TB drugs, or later age at first sex among young people. These positive short-term outcomes should lead to changes in the longer term impact of programmes, measured in fewer new cases of HIV, TB, or malaria. In the case of HIV, a desired impact among those infected includes quality of life and life expectancy.</p>
                        <p>Measuring impact requires extensive investment in evaluation, and it is often difficult to ascertain the extent to which individual programmes, or individual programme components, contribute to overall reduction in cases and increased survival. In order to establish a cause-effect relationship for a given intervention, studies with experimental or quasi experimental designs may be necessary to demonstrate the impact. Outputs or outcome indicators however, can also be used to a certain degree to identify such relationships and can give a general indication of programmes progress according to agreed upon goals and targets.</p>
                        <ul class="enhanced-list">
                            <li>Generic input indicator: Existence of national policies, guidelines, or strategies. This is a "yes" / "no" question. Reporting of overall budget allocation is included as an input.</li>
                            <li>Generic process indicator: Number of persons trained, number of drugs shipped/ordered, etc. In the case of HIV/AIDS, for example, these include prevention, treatment, care and</li>
                        </ul>

                        <h5>Examples of cross cutting indicators for HIV/AIDS and TB Programmes</h5>
                        <h5>Health systems strengthening</h5>
                        <ul class="enhanced-list">
                            <li>Number of project staff trained</li>
                            <li>% of project budget spent on health infrastructure</li>
                            <li>% of project beneficiaries (patients) who are accurately referred</li>
                        </ul>
                        <p>Coordination and partnership development</p>
                        <ul class="enhanced-list">
                            <li>Number of networks/partnerships involved in project</li>
                        </ul>
                        <p>Monitoring and evaluation</p>
                        <ul class="enhanced-list">
                            <li>Number of project service deliverers trained in M&E</li>
                            <li>% of overall project budget spent on M&E</li>
                        </ul>
                        <p>Procurement and supply management capacity building</p>
                        <ul class="enhanced-list">
                            <li>Number of project services deliverers trained in procurement and supply management</li>
                            <li>% of project service delivery points with sufficient drug supplies</li>
                            <li>Unit cost(s) of project drug(s) and commodities</li>
                        </ul>

                        <h5>- Standard Indicators</h5>
                        <h5>Why do we need standard indicators?</h5>
                        <p>The use of standard indicators provides the National Programme with valuable measures of the same indicator in different populations, permitting triangulation of findings and allowing regional or local inconsistencies and differences to be noted and addressed. This helps to direct resources to regions or sub-populations with greater needs and to identify areas for intensification or reduction of effort at the national level, ultimately improving the overall effectiveness of the national response. The use of standard indicators also ensures comparability of information across countries and over time.</p>
                        <p>In designing their own evaluation activities, projects should also bear in mind the national standard for indicators in that field. Projects may have their own information needs that conform to a rigorous evaluation design. However, whenever possible they should choose indicators with standard references, e.g. reference periods, numerators, denominators collected consistently over various time periods that would allow the data they collect to be fed easily into the national M&E system, and compared over time.</p>

                        <h5>- Choices of indicators</h5>
                        <p>One of the critical steps in designing and carrying out M&E of any program for that matter is selecting appropriate indicators. This can be a fairly straightforward process if the objectives of the program have been clearly stated and presented in terms that define quantity, quality, and time frame of a particular aspect of the program. Even with welldefined objectives, however, the choice of indicators for the evaluation of many programs requires careful thought and consideration of both theoretical and practical elements. The following questions can be helpful in selecting indicators:</p>
                        <ul class="enhanced-list">
                            <li>Is the focus of the objective a parameter that can be measured accurately and reliably?</li>
                            <li>Are there alternative measures that need to be considered?</li>
                            <li>What resources (human and financial) does the indicator require?</li>
                            <li>Are there areas for congruency, either in the content of the indicator or the means of gathering the data?</li>
                            <li>Are there any additional measures that would help in interpreting the results of the primary objective?</li>
                        </ul>
                        <p>Selecting indicators and setting targets is usually done during the process of program planning and replanning, preferably in a participatory way with the implementing agency and key stakeholders. Setting targets and benchmarks should also include information from similar types of interventions, so that the targets set are realistic from the perspective of the target population, resource allocation, and intervention type. While the level of attainment to be measured by the indicator is not actually part of the indicator itself, it is a critical factor. The magnitude of the level to be measured affects the size of the sample of the population needed to estimate that level accurately. It may also help evaluators select additional or supplemental indicators that might assist in later interpretations of the results.</p>
                        <p>The choice of indicators should be therefore driven by the goals of the national programme or project. There is no point in collecting data on areas that are not relevant to the local context, bearing in mind that it costs time and money to collect and analyze data for each indicator.</p>

                        <h5>- Guiding principles in choosing a set of indicator</h5>
                        <p>The following guiding principles help in choosing the most appropriate set of indicators and associated data collection instruments:</p>
                        <ol class="enhanced-list">
                            <li>Use a conceptual framework for M&E for proper interpretation of the results (see above for suggested framework);</li>
                            <li>Ensure that the indicators are linked to the programme or project goals and are able to measure change;</li>
                            <li>Ensure that standard indicators are used to the extent possible for comparability over time and between countries or population groups;</li>
                            <li>Consider the cost and feasibility of data collection and analysis; and</li>
                            <li>For HIV/AIDS, take into account the stage of the epidemic</li>
                            <li>Keep the number of indicators to the minimum needed, with specific reference to the level of the system that require and will use which indicators to make programming and management decisions. Additional indicators can always be identified later.</li>
                        </ol>
                        <p>The Table below lists possible indicators related to different levels of program evaluation. The advantage of relating indicators to specific evaluation levels is that it also helps to identify opportunities for triangulating data.</p>

                        <div class="table-container">
                            <table class="content-table">
                                <caption>Examples of indicators by level of program evaluation</caption>
                                <thead>
                                    <tr>
                                        <th>Levels of Evaluation</th>
                                        <th>Indicator</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td colspan="2"><strong>1. Process Evaluation</strong></td>
                                    </tr>
                                    <tr>
                                        <td>INPUTS</td>
                                        <td>
                                            <ul>
                                                <li>Resources allocated (e.g., percent of national budget)</li>
                                                <li>Condom availability at central level (Prevention Indicator</li>
                                            </ul>
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>OUTPUTS</td>
                                        <td>
                                            <ul>
                                                <li>Knowledge of HIV transmission (PI 1)</li>
                                                <li>Condom availability in periphery (PI 3)</li>
                                                <li>Proportion of those 12-17 years of age receiving AIDS/sexual health education</li>
                                                <li>Percentage of services with improved quality, e.g., STI case management (PI 6 and PI 7), voluntary counseling and testing (VCT), care for people living with HIV/AIDS</li>
                                                <li>Percentage of blood transfusion facilities with uninterrupted supply of appropriate HIV screening tests</li>
                                            </ul>
                                        </td>
                                    </tr>
                                    <tr>
                                        <td colspan="2"><strong>2. Effectiveness Evaluation</strong></td>
                                    </tr>
                                    <tr>
                                        <td>OUTCOMES</td>
                                        <td>
                                            <ul>
                                                <li>Condom use during last act with non-regular partner</li>
                                                <li>Prevalence of urethritis among men aged 15-49 in last year</li>
                                                <li>Prevalence of positive syphilis (RPR/VDRL) serology among antenatal women aged 15-24</li>
                                            </ul>
                                        </td>
                                    </tr>
                                    <tr>
                                        <td>IMPACT (HIV incidence/prevalence/mitigation)</td>
                                        <td>
                                            <ul>
                                                <li>HIV prevalence among women less than 25 years of age in antenatal clinics (to approximate incidence)</li>
                                                <li>HIV prevalence among high risk groups, e.g., STI patients/sex workers/injecting drug users</li>
                                                <li>HIV prevalence among adult men aged 15-49</li>
                                                <li>HIV-associated mortality rates among adults aged 15-59</li>
                                                <li>Number of communities with increased coping capacity</li>
                                            </ul>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h5>RATIONALE, NUMERATOR AND DENOMINATOR OF COVERAGE INDICATORs</h5>
                        <h5>✓ Definition</h5>
                        <p>Indicators are markers designed to monitor and evaluate program performance/ effectiveness or goals/objectives. Indicator types are: input, output, outcome and impact based on life cycle of a program.</p>

                        <h5>Seven characteristics of an ideal indicator are:</h5>
                        <ol class="enhanced-list">
                            <li>Directly related to program objectives,</li>
                            <li>Operational (measurable),</li>
                            <li>Reliable</li>
                            <li>Valid</li>
                            <li>Specific,</li>
                            <li>Sensitive</li>
                            <li>Affordable and feasible.</li>
                        </ol>

                        <h5>- Significance of standard indicators</h5>
                        <p>The use of standard indicators provides the National Programme with valuable measures of the same indicator in different populations,</p>
                        <ul class="enhanced-list">
                            <li>permitting triangulation of findings and</li>
                            <li>allowing regional or local inconsistencies and differences to be noted and addressed.</li>
                            <li>ensures comparability of information across countries and over time.</li>
                        </ul>
                        <p>This helps to direct resources to regions or sub-populations with greater needs and to identify areas for intensification or reduction of effort at the national level, ultimately improving the overall effectiveness of the national response. In designing their own evaluation activities, projects should therefore bear in mind the national standard for indicators in that field. Projects may have their own information needs that conform to a rigorous evaluation design. However, whenever possible they should choose indicators with standard references, e.g. reference periods, numerators, denominators collected consistently over various time periods that would allow the data they collect to be fed easily into the national M&E system, and compared over time.</p>

                        <h5>- Coverage indicators</h5>
                        <h5>Example 1: Percentage of sex workers reached with HIV prevention programs (HIV-C-P2) for HIV prevention among sex workers.</h5>
                        <h5>Rationale</h5>
                        <p>It measures progress in implementing basic elements of HIV prevention programmes for sex workers. Sex workers are often difficult to reach with HIV prevention programmes. However, in order to prevent the spread of HIV and AIDS among sex workers as well as into the general population, it is important that they access these services. Note: Countries with generalized epidemics may also have a concentrated sub-epidemic among one or more most-at-risk populations. If so, they should calculate and report this indicator for those populations.</p>
                        <h5>Numerator</h5>
                        <p>Number sex workers who replied "yes" to both questions:</p>
                        <ol class="enhanced-list">
                            <li>Do you know where you can go if you wish to receive an HIV test?</li>
                            <li>In the last twelve months, have you been given condoms?</li>
                        </ol>
                        <h5>Denominator</h5>
                        <p>Total number of sex workers surveyed</p>
                        <h5>Measurement</h5>
                        <p>Behavioural surveillance or other special surveys. Respondents are asked the following questions:</p>
                        <ol class="enhanced-list">
                            <li>Do you know where you can go if you wish to receive an HIV test?</li>
                            <li>In the last twelve months, have you been given condoms? (e.g. through an outreach service, drop-in centre or sexual health clinic)</li>
                        </ol>
                        <p>Scores for each of the individual questions-based on the same denominator-are required in addition to the score for the composite indicator. Whenever possible, data for sex workers should be collected through civil society organizations that have worked closely with this population in the field. Access to survey respondents as well as the data collected from them must remain confidential. This indicator only covers two basic elements of prevention programmes for sex workers. It is recognized that the indicator does not measure the frequency with which members of these populations access services, nor the quality of these services. These limitations suggest that the indicator may overestimate the coverage of HIV prevention services or sex workers. While continued monitoring of this indicator is recommended in order to determine trends in coverage of minimum services, additional measures are required in order to accurately determine whether adequate HIV prevention services are being provided for these populations.</p>

                        <h4>1.3.8 Goals and objectives of Health programs' M&E system</h4>
                        <p>The goals and objectives of HIV/TB program are the core of any M &E system because: (i) M&E is a process of data collection and measurement of progress/impact toward program goals/objectives while evaluation is a systematic investigation of a program's effectiveness in line with the objectives (ii) Goals or objectives are therefore an essential component in quantifying the aims of HIV/AIDS & TB related policies, programs and services through M&E systems. Five features of smart objectives in a project are: $\mathrm{S}=$ Specificity, $\mathrm{M}=$ Measurability, $\mathrm{A}=$ Attainability, $\mathrm{R}=$ Reliability and $\mathrm{T}=$ Time bound</p>

                        <h4>1.3.9 Data Collection</h4>
                        <h5>- Data vs Information</h5>
                        <p>A mixed qualitative and quantitative approach should be applied when collecting data and analyzing information. The mixed methodological approach will contribute to a more substantial understanding of programme progress, ensure triangulation of data sources and reduce biases in the data.</p>
                        <p>Different methods of collecting data for M&E: These include quantitative & qualitative methods / related instruments or tools = questionnaires, observations/checklist, FGDs, etc. All methods require participatory approach.</p>
                        <p>Planning data collection for selected indicators requires different strategies because:</p>
                        <ul class="enhanced-list">
                            <li>The cost, difficulty, and capacity required for collecting information increases as indicators shift from input through outputs and from outcomes to impact.</li>
                            <li>The reverse is true when assessing the impact of program related interventions, which decrease as indicators move from input to impact.</li>
                            <li>Input and output indicators are often easy and less costly to collect compared to outcome and impact indicators.</li>
                            <li>Data for input and output indicators are centrally collected from regular health monitoring systems, provided that such systems are functional. While outcome and impact indicator data are collected through more costly and difficult population-based or health facility surveys, requiring some expertise in research methods. Based on these reasons, planning data collection for selected indicators requires different strategies.</li>
                        </ul>

                        <h5>Optimizing the use of data for any disease programming</h5>
                        <p>The ultimate goal of data collection is to ensure that data are fed back into the decisionmaking process. In addition, data are powerful tools for advocacy, generating resources, accountability, program design and improvement, and attributing changes to specific interventions. The following steps can help to optimize the use of data:</p>
                        <ol class="enhanced-list">
                            <li>Produce quality data;</li>
                            <li>Identify the different end-users and present and package the data according to their needs, focusing on a minimal number of indicators at each level;</li>
                            <li>Set up mechanism for an efficient data-use system, including feedback through supervision at all levels an assurance that data at given level is relevant and actionable at that level;</li>
                            <li>Ensure ownership throughout the data collection exercise, which means that national and local M&E capacities must be strengthened to guarantee uniform and quality data within a sustainable framework;</li>
                            <li>Ensure that an M&E support group with strong presence from the government, donor agencies, NGOs, and academic institutions is established to guide the government throughout the development and implementation of national M&E strategies. This will improve the credibility of the data generated by the government and</li>
                            <li>Allocate sufficient resources for the development and implementation of a data use-plan</li>
                        </ol>
                    </div>
                    <!-- END: 1.3 -->
                </section>
                <!-- END: topic-1 -->
            </main>

            <!-- Bottom Navigation -->
             <nav class="document-nav" role="navigation" aria-label="Document Sections">
                <div class="nav-links">
                    <a href="../index.html" class="nav-button">
                        <span class="nav-icon"></span>
                        <span class="nav-text">Table of Contents</span>
                    </a>
                    <a href="#" class="nav-button disabled">
                        <span class="nav-icon">←</span>
                        <span class="nav-text">Previous Section</span>
                    </a>
                    <div class="document-progress">
                         <div class="progress-bar" aria-hidden="true">
                            <div class="progress-fill" style="width: 25%;"></div>
                        </div>
                       <span class="progress-text">Topic 1 of 4</span>
                    </div>
                    <a href="02-system-and-framework.html" class="nav-button">
                        <span class="nav-text">Next Section</span>
                        <span class="nav-icon">→</span>
                    </a>
                </div>
            </nav>

        </article>
    </div>
    <script src="../js/navigation.js"></script>
</body>
</html>
