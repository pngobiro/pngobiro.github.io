<!doctype html>
<html lang="en">
<head>
<link rel="stylesheet" type="text/css" href="base.css" />
<link rel="stylesheet" type="text/css" href="content.css" />
<link rel="stylesheet" type="text/css" href="nav.css" />
<meta http-equiv="content-type" content="text/html;  charset=utf-8" />
<title>November 2017 </title>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<meta name="generator" content="eXeLearning 2.9 - exelearning.net" />
<!--[if lt IE 9]><script type="text/javascript" src="exe_html5.js"></script><![endif]-->
<script type="text/javascript" src="exe_jquery.js"></script>
<script type="text/javascript" src="common_i18n.js"></script>
<script type="text/javascript" src="common.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="exe-web-site" id="exe-node-6"><script type="text/javascript">document.body.className+=" js"</script>
<div id="content">
<p id="skipNav"><a href="#main" class="sr-av">Skip navigation</a></p>
<section id="emptyHeader"></section>
<nav id="siteNav">
<ul>
   <li><a href="index.html" class="daddy main-node">Home</a></li>
   <li><a href="july_2023.html" class="no-ch">July 2023</a></li>
   <li><a href="november_2021.html" class="no-ch">November 2021</a></li>
   <li><a href="july_2021.html" class="no-ch">July 2021</a></li>
   <li><a href="july_2019.html" class="no-ch">July 2019</a></li>
   <li><a href="november_2018.html" class="no-ch">November 2018</a></li>
   <li id="active"><a href="november_2017.html" class="active no-ch">November 2017</a></li>
   <li><a href="july_2017.html" class="no-ch">July 2017</a></li>
   <li><a href="november_2016.html" class="no-ch">November 2016</a></li>
   <li><a href="july_2016.html" class="no-ch">July 2016</a></li>
   <li><a href="november_2015.html" class="no-ch">November 2015</a></li>
   <li><a href="july_2015.html" class="no-ch">July 2015</a></li>
   <li><a href="novemeber_2014.html" class="no-ch">Novemeber 2014</a></li>
   <li><a href="july_2014.html" class="no-ch">July 2014</a></li>
   <li><a href="november_2013.html" class="no-ch">November 2013</a></li>
   <li><a href="july_2013.html" class="no-ch">July 2013</a></li>
   <li><a href="november_2012.html" class="no-ch">November 2012</a></li>
   <li><a href="july_2012.html" class="no-ch">July 2012</a></li>
   <li><a href="july_2011.html" class="no-ch">July 2011</a></li>
</ul>
</nav>
<div id='topPagination'>
<nav class="pagination noprt">
<a href="november_2018.html" class="prev"><span><span>&laquo; </span>Previous</span></a> <span class="sep">| </span><a href="july_2017.html" class="next"><span>Next<span> &raquo;</span></span></a>
</nav>
</div>
<div id="main-wrapper">
<section id="main">
<header id="nodeDecoration"><h1 id="nodeTitle">November 2017</h1></header>
<article class="iDevice_wrapper textIdevice" id="id6">
<div class="iDevice emphasis0" >
<div id="ta6_128_2" class="block iDevice_content">
<div class="exe-text"><p></p>
<style>
        /* Modern CSS Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Base Styles */
        body {
            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            color: #2d3748;
            background-color: #f7fafc;
            margin: 0;
            padding: 0;
        }

        /* Header Styles */
        header {
            background: linear-gradient(135deg, #1a365d 0%, #2c5282 100%);
            color: white;
            padding: 2rem;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        header h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            font-weight: 600;
        }

        .exam-details {
            background: rgba(255, 255, 255, 0.1);
            padding: 1rem;
            border-radius: 8px;
            display: inline-block;
            margin-top: 1rem;
        }

        .exam-details p {
            margin: 0.5rem 0;
            font-size: 1.1rem;
        }

        /* Main Content */
        main {
            max-width: 1200px;
            margin: 2rem auto;
            padding: 0 1.5rem;
        }

        /* Passage Styles */
        .passage {
            background: #edf2f7;
            border-radius: 12px;
            margin-bottom: 2rem;
            padding: 1.5rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .passage h2 {
            color: #2d3748;
            margin-bottom: 1rem;
            font-size: 1.5rem;
            border-bottom: 2px solid #cbd5e0;
            padding-bottom: 0.5rem;
        }

        .passage-content {
            font-size: 1.1rem;
            color: #4a5568;
            line-height: 1.8;
        }

        .passage-content p {
            margin-bottom: 1rem;
        }

        /* Question Styles */
        .question {
            background: white;
            border-radius: 12px;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            overflow: hidden;
        }

        .question h3 {
            background: #4a5568;
            color: white;
            padding: 1rem 1.5rem;
            font-size: 1.25rem;
            margin: 0;
            border-bottom: 3px solid #2d3748;
        }

        .question-content {
            padding: 1.5rem;
            background: #fff;
            border-bottom: 2px solid #edf2f7;
        }

        .question-content p {
            font-size: 1.1rem;
            color: #2d3748;
        }

        /* Answer Styles */
        .answer-section {
            background: #f8fafc;
            border-top: 2px solid #e2e8f0;
        }

        .answer-section h4 {
            padding: 1rem 1.5rem;
            color: #2d3748;
            font-size: 1.2rem;
            background: #edf2f7;
            margin: 0;
        }

        .answer-content {
            padding: 1.5rem;
            color: #4a5568;
        }

        .answer-content p {
            margin-bottom: 1rem;
            line-height: 1.8;
        }

        /* Bold Answers */
        .answer-content strong, .answer-content b {
            color: #2d3748;
            font-weight: 700;
        }

        /* Numbered Lists in Answers */
        .answer-content p[data-number]:before {
            content: attr(data-number) ".";
            font-weight: 600;
            margin-right: 0.5rem;
            color: #4a5568;
        }

        /* Table Styles */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            overflow: hidden;
            border-radius: 8px;
        }

        thead {
            background-color: #4a5568;
            color: white;
        }

        th {
            text-align: left;
            padding: 0.75rem 1rem;
            font-weight: 600;
            border-bottom: 2px solid #2d3748;
        }

        td {
            padding: 0.75rem 1rem;
            border-bottom: 1px solid #e2e8f0;
            background-color: white;
        }

        tr:last-child td {
            border-bottom: none;
        }

        tr:nth-child(even) td {
            background-color: #f8fafc;
        }

        /* Footer Styles */
        footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 1.5rem;
            margin-top: 3rem;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            header h1 {
                font-size: 2rem;
            }

            main {
                padding: 0 1rem;
            }

            .passage h2 {
                font-size: 1.3rem;
            }

            .passage-content p {
                font-size: 1rem;
            }

            .question {
                margin-bottom: 1.5rem;
            }

            .question h3 {
                font-size: 1.1rem;
            }

            .question-content p,
            .answer-content p {
                font-size: 1rem;
            }
            
            table, thead, tbody, th, td, tr {
                display: block;
            }
            
            thead tr {
                position: absolute;
                top: -9999px;
                left: -9999px;
            }
            
            tr {
                border: 1px solid #e2e8f0;
                margin-bottom: 1rem;
                border-radius: 8px;
                overflow: hidden;
            }
            
            td {
                border: none;
                position: relative;
                padding-left: 50%;
                text-align: left;
                border-bottom: 1px solid #e2e8f0;
            }
            
            td:before {
                position: absolute;
                top: 0.75rem;
                left: 1rem;
                width: 45%;
                padding-right: 10px;
                white-space: nowrap;
                font-weight: 600;
                content: attr(data-label);
            }
            
            td:last-child {
                border-bottom: 0;
            }
        }

        /* Print Styles */
        @media print {
            body {
                background: white;
            }

            .passage,
            .question {
                break-inside: avoid;
                box-shadow: none;
                border: 1px solid #edf2f7;
            }

            header {
                background: white;
                color: black;
                padding: 1rem;
            }

            .exam-details {
                border: 1px solid #edf2f7;
            }
            
            table {
                break-inside: auto;
            }
            
            tr {
                break-inside: avoid;
                break-after: auto;
            }
        }
    </style>
<header>
<h1>OPERATING SYSTEMS</h1>
<div class="exam-details">
<p>Exam Code: 2920/105</p>
<p>Duration: 3 hours</p>
<p>Period: November 2017</p>
</div>
</header>
<p><main>
<section class="question">
<h3>1. (3 marks)</h3>
<div class="question-content">
<p>(a) Outline three functions of the job control language in a computer system.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Job Submission</strong>: JCL provides a means for users to submit jobs to the operating system for processing. It acts as the interface through which users communicate their processing requests.</p>
<p data-number="2"><strong>Resource Allocation</strong>: JCL allows users to specify the resources required for their jobs, such as memory, input/output devices, and processing time. This enables the operating system to allocate resources efficiently and manage job execution.</p>
<p data-number="3"><strong>Job Control</strong>: JCL enables users to control the execution sequence of programs within a job. It defines the steps to be performed, including the programs to be executed, the data files to be used, and the desired output.</p>
</div>
</div>
</section>
<section class="question">
<h3>1. (6 marks)</h3>
<div class="question-content">
<p>(b) Explain each of the following terms as used in operating systems:</p>
<p>(i) spooling;</p>
<p>(ii) warm boot;</p>
<p>(iii) context switch.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Spooling</strong>:</p>
<p><strong>Spooling</strong> (Simultaneous Peripheral Operations On-Line) is a process in which data is temporarily held in a buffer to be used and executed by a device, program or the system. Data is sent to and stored in temporary storage such as a disk or memory for execution at a later time. Spooling is useful because devices access data at different rates. For example, a printer is much slower than a CPU. Spooling allows the CPU to continue processing other jobs while the printer is working, improving overall system efficiency.</p>
<p><strong>(ii) Warm boot</strong>:</p>
<p>A <strong>warm boot</strong>, also known as a soft reboot, is the process of restarting a computer system without interrupting the power supply. In a warm boot, the operating system is instructed to restart, typically through a software command or a system menu option. The system goes through a restart sequence, but it doesn't power down completely. It is generally faster than a cold boot as some system components may retain their state. However, it may not resolve issues that require a complete power cycle.</p>
<p><strong>(iii) Context switch</strong>:</p>
<p>A <strong>context switch</strong> is the procedure followed by the operating system kernel to switch the CPU from one process to another. It involves saving the state of the current process (including the program counter, registers, and memory management information) and loading the saved state of the new process to be executed. Context switching is essential for multitasking operating systems, allowing multiple processes to share the CPU and appear to run concurrently. It is a fundamental operation for achieving concurrency and responsiveness in modern operating systems.</p>
</div>
</div>
</section>
<section class="question">
<h3>1. (7 marks)</h3>
<div class="question-content">
<p>(c) A disk has the following head movement queue 95, 190, 40, 115, 18, 123, 52, 60 with the read-write head initially at track 40 and the tail track at 210. With the aid of a Gantt graph, determine the total seek time using each of the following disk scheduling algorithms:</p>
<p>(i) shortest seek time first;</p>
<p>(ii) elevator assuming the head moves towards 0.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Shortest Seek Time First (SSTF)</strong>:</p>
<p><strong>Gantt Chart for SSTF:</strong></p>
<p>[40] - 52 - 60 - 40 - 18 - 95 - 115 - 123 - 190</p>
<p><strong>Seek Sequence:</strong> 40, 52, 60, 40, 18, 95, 115, 123, 190</p>
<p><strong>Seek Time Calculation:</strong></p>
<p>|52 - 40| + |60 - 52| + |40 - 60| + |18 - 40| + |95 - 18| + |115 - 95| + |123 - 115| + |190 - 123|</p>
<p>= 12 + 8 + 20 + 22 + 77 + 20 + 8 + 67 = 234</p>
<p><strong>Total seek time for SSTF is 234 track movements.</strong></p>
<p><strong>(ii) Elevator (SCAN) assuming the head moves towards 0</strong>:</p>
<p><strong>Gantt Chart for Elevator (SCAN):</strong></p>
<p>[40] - 18 - 40 - 52 - 60 - 95 - 115 - 123 - 190</p>
<p><strong>Seek Sequence:</strong> 40, 18, 40, 52, 60, 95, 115, 123, 190</p>
<p><strong>Seek Time Calculation:</strong></p>
<p>|18 - 40| + |40 - 18| + |52 - 40| + |60 - 52| + |95 - 60| + |115 - 95| + |123 - 115| + |190 - 123|</p>
<p>= 22 + 22 + 12 + 8 + 35 + 20 + 8 + 67 = 194</p>
<p><strong>Total seek time for Elevator (SCAN) is 194 track movements.</strong></p>
</div>
</div>
</section>
<section class="question">
<h3>1. (4 marks)</h3>
<div class="question-content">
<p>(d) Explain two reasons why application programs are temporarily stored in the main memory of a computer system.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>CPU Access Speed</strong>: Main memory (RAM) offers significantly faster access speeds compared to secondary storage (like hard disks or SSDs). The CPU can fetch instructions and data from RAM much more quickly, which is crucial for program execution. Temporarily storing application programs in main memory allows the CPU to access them rapidly, leading to faster program execution and improved system performance.</p>
<p data-number="2"><strong>Execution Requirement</strong>: Application programs need to be in main memory to be executed by the CPU. The CPU can only directly access instructions and data that are present in the main memory. When an application is launched, its executable code and necessary data are loaded from secondary storage into main memory. This loading process is essential because the CPU fetches and executes instructions sequentially from main memory to run the program.</p>
</div>
</div>
</section>
<section class="question">
<h3>2. (6 marks)</h3>
<div class="question-content">
<p>(a) Outline three examples of systems calls for each of the following:</p>
<p>(i) file manipulation;</p>
<p>(ii) communication.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) File Manipulation System Calls:</strong></p>
<p data-number="1"><strong>`open()`</strong>: This system call is used to <strong>open a file</strong>. It typically takes the filename and mode (read, write, append) as arguments and returns a file descriptor, which is used for subsequent operations on the file.</p>
<p data-number="2"><strong>`read()`</strong>: The `read()` system call is used to <strong>read data from a file</strong>. It takes the file descriptor, a buffer to store the data, and the number of bytes to read as arguments. It returns the number of bytes actually read, which may be less than requested.</p>
<p data-number="3"><strong>`write()`</strong>: This system call is used to <strong>write data to a file</strong>. It takes the file descriptor, a buffer containing the data to be written, and the number of bytes to write as arguments. It returns the number of bytes actually written.</p>
<p><strong>(ii) Communication System Calls:</strong></p>
<p data-number="1"><strong>`socket()`</strong>: The `socket()` system call is used to <strong>create a new socket</strong>, which is an endpoint for communication. It takes arguments specifying the address family, socket type, and protocol. It returns a file descriptor representing the socket.</p>
<p data-number="2"><strong>`send()`</strong>: The `send()` system call is used to <strong>transmit data to another socket</strong>. It takes the socket descriptor, a buffer containing the data to be sent, and the length of the data as arguments. It is typically used for connection-oriented sockets (e.g., TCP).</p>
<p data-number="3"><strong>`recv()`</strong>: The `recv()` system call is used to <strong>receive data from a socket</strong>. It takes the socket descriptor, a buffer to store the received data, and the maximum number of bytes to receive as arguments. It returns the number of bytes received.</p>
</div>
</div>
</section>
<section class="question">
<h3>2. (4 marks)</h3>
<div class="question-content">
<p>(b) Explain two modes of processing supported by multiprocessing operating system.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Symmetric Multiprocessing (SMP)</strong>: In <strong>SMP</strong>, multiple processors share the system memory and I/O resources. Each processor runs an identical copy of the operating system, and they can execute different processes or threads concurrently. SMP provides true parallelism and is commonly used in systems requiring high performance and throughput. The processors are treated equally, and the OS manages the distribution of tasks across them.</p>
<p data-number="2"><strong>Asymmetric Multiprocessing (AMP)</strong>: In <strong>AMP</strong>, processors are assigned specific tasks. Typically, one processor acts as the master processor, running the operating system and managing the system, while other processors (slave processors) are dedicated to specific tasks or applications. AMP is simpler to implement than SMP but offers less flexibility and scalability. It's often used in embedded systems or specialized applications where tasks can be clearly partitioned and assigned to different processors.</p>
</div>
</div>
</section>
<section class="question">
<h3>2. (2 marks)</h3>
<div class="question-content">
<p>(c) Several programs can be run simultaneously using a multiprogramming operating system.</p>
<p>(i) Explain the objective of this mode of processing.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Objective of Multiprogramming</strong>:</p>
<p>The primary objective of multiprogramming is to <strong>maximize CPU utilization</strong> and <strong>increase system throughput</strong>. By keeping multiple programs in memory, the operating system can switch the CPU to another program whenever the current program is waiting for I/O operations to complete. This prevents the CPU from sitting idle and ensures that it is always working on some task, thus improving overall system efficiency and responsiveness.</p>
</div>
</div>
</section>
<section class="question">
<h3>2. (2 marks)</h3>
<div class="question-content">
<p>(ii) Explain the problem associated with this mode of processing.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(ii) Problem associated with Multiprogramming</strong>:</p>
<p>One of the main problems associated with multiprogramming is <strong>increased memory management complexity</strong>. Keeping multiple programs in memory simultaneously requires sophisticated memory management techniques to prevent programs from interfering with each other, to allocate memory efficiently, and to handle swapping or paging when memory becomes full. This complexity adds overhead to the operating system and can potentially lead to performance issues if not managed effectively.</p>
</div>
</div>
</section>
<section class="question">
<h3>2. (6 marks)</h3>
<div class="question-content">
<p>(d) Joseph intends to acquire an operating system for his company's network system. Explain three factors he should consider other than cost.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Security Features</strong>: Joseph should consider the <strong>security features</strong> offered by the operating system. This includes evaluating the OS's vulnerability to malware, its firewall capabilities, intrusion detection/prevention systems, and user access control mechanisms. A secure operating system is crucial for protecting sensitive company data and maintaining the integrity of the network system.</p>
<p data-number="2"><strong>Scalability and Performance</strong>: The operating system should be <strong>scalable</strong> to handle future growth in network size and user load. Joseph needs to assess the OS's performance under heavy network traffic, its ability to manage a large number of concurrent connections, and its efficiency in resource utilization. Performance and scalability are vital for ensuring the network system remains responsive and efficient as the company expands.</p>
<p data-number="3"><strong>Compatibility and Support</strong>: Joseph must ensure the operating system is <strong>compatible</strong> with the existing hardware and software infrastructure within his company. He also needs to consider the availability of technical support, updates, and a strong community for the chosen OS. Good compatibility minimizes integration issues, while reliable support ensures timely resolution of any operational problems and long-term system maintainability.</p>
</div>
</div>
</section>
<section class="question">
<h3>3. (6 marks)</h3>
<div class="question-content">
<p>(a) The operating system uses various approaches of interfacing with users. Describe three such approaches.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Command Line Interface (CLI)</strong>: A <strong>CLI</strong> allows users to interact with the operating system by typing text-based commands. Users enter commands at a prompt, and the system responds with text output. CLIs are efficient and powerful for experienced users, offering precise control over the system. Examples include the terminal in Linux/macOS and Command Prompt in Windows.</p>
<p data-number="2"><strong>Graphical User Interface (GUI)</strong>: A <strong>GUI</strong> provides a visual way for users to interact with the operating system using icons, menus, and windows. Users interact using a mouse, keyboard, or touch input. GUIs are user-friendly and intuitive, making computers accessible to a wider range of users. Examples include Windows desktop, macOS Finder, and GNOME or KDE in Linux.</p>
<p data-number="3"><strong>Batch Interface</strong>: In a <strong>batch interface</strong>, users prepare a batch of commands or jobs in a file and submit it to the operating system for execution. The system processes the batch without requiring user interaction during execution. Batch interfaces are suitable for automating repetitive tasks and processing large volumes of data. They were common in early operating systems and are still used for tasks like overnight processing or scheduled jobs.</p>
</div>
</div>
</section>
<section class="question">
<h3>3. (4 marks)</h3>
<div class="question-content">
<p>(b) Explain each of the modes of programmable clocks as used in I/O device management:</p>
<p>(i) one-shot;</p>
<p>(ii) square-wave.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) One-Shot Mode</strong>:</p>
<p>In <strong>one-shot mode</strong>, the programmable clock is set to generate a single pulse or interrupt after a specified time interval. Once the timer counts down to zero and generates the interrupt, it stops and needs to be re-armed or re-initialized to trigger again. This mode is useful for tasks that require a single timed event, such as setting a delay or triggering an action after a specific duration. After the event, the clock becomes inactive until explicitly restarted.</p>
<p><strong>(ii) Square-Wave Mode</strong>:</p>
<p>In <strong>square-wave mode</strong>, the programmable clock is configured to generate a continuous, periodic sequence of pulses or interrupts. Once started, the timer repeatedly counts down to zero and generates an interrupt, then automatically restarts the countdown from the initial value. This creates a square wave output, where the frequency and duty cycle can be configured. Square-wave mode is suitable for tasks that require periodic events, such as real-time clock functions, generating PWM signals, or scheduling recurring tasks at regular intervals.</p>
</div>
</div>
</section>
<section class="question">
<h3>3. (4 marks)</h3>
<div class="question-content">
<p>(c) Distinguish between the CPU bound and I/O bound jobs during inter process communication.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>CPU-Bound Jobs</strong>:</p>
<p><strong>CPU-bound jobs</strong> are processes where the rate at which the process progresses is primarily limited by the speed of the CPU. These jobs require significant processing power and spend most of their time performing computations in the CPU. In inter-process communication, CPU-bound jobs might be involved in intensive data processing or calculations before or after communication, but the communication itself is not the bottleneck. The performance of CPU-bound jobs heavily depends on CPU speed and efficiency.</p>
<p><strong>I/O-Bound Jobs</strong>:</p>
<p><strong>I/O-bound jobs</strong> are processes where the rate of progress is limited by the speed of I/O operations, such as reading from or writing to disks, network communication, or user input. These jobs spend a significant portion of their time waiting for I/O operations to complete. In inter-process communication, I/O-bound jobs might be waiting for data to be received from another process, or they might be sending data that takes time to be transmitted. The communication aspect is more central to the job's performance compared to CPU-bound jobs. Performance of I/O-bound jobs is more dependent on I/O subsystem speed and efficiency of communication channels.</p>
</div>
</div>
</section>
<section class="question">
<h3>3. (6 marks)</h3>
<div class="question-content">
<p>(d) Files can be stored on a directory using various logical structures. Describe three such structures that the operating system supports.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Single-Level Directory</strong>: A <strong>single-level directory structure</strong> is the simplest form, where all files are placed in a single directory. This structure is easy to implement but has limitations, especially when the number of files increases. It can lead to naming conflicts if two users want to use the same filename, and it becomes difficult to organize and manage a large number of files. It is rarely used in modern operating systems due to its lack of scalability and organization.</p>
<p data-number="2"><strong>Two-Level Directory</strong>: A <strong>two-level directory structure</strong> introduces a hierarchy by having a master directory and user-level directories. Each user has their own directory under the master directory, and all files belonging to a user are placed in their respective user directory. This structure solves the naming conflict issue of the single-level directory and provides some level of organization. However, it still lacks flexibility for users to further organize their files within their own directories.</p>
<p data-number="3"><strong>Tree-Structured Directory</strong>: A <strong>tree-structured directory</strong> (or hierarchical directory) is the most common and flexible structure used in modern operating systems. It allows users to create directories within directories, forming a tree-like hierarchy. This structure provides a high degree of organization and allows users to create a logical grouping of files and directories. It supports efficient file searching and management and is widely used in systems like Unix, Linux, and Windows. Users can create a directory hierarchy that mirrors their organizational needs.</p>
</div>
</div>
</section>
<section class="question">
<h3>4. (3 marks)</h3>
<div class="question-content">
<p>(a) Outline three objectives of I/O scheduling in a computer system.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Maximize Throughput</strong>: One key objective of I/O scheduling is to <strong>maximize the number of I/O requests processed per unit of time (throughput)</strong>. Efficient scheduling algorithms aim to reduce the total time spent in servicing I/O requests, allowing more requests to be completed in a given period. This improves the overall performance of the system by reducing I/O bottlenecks.</p>
<p data-number="2"><strong>Minimize Response Time</strong>: Another important objective is to <strong>minimize the response time for I/O requests</strong>. Response time is the delay between issuing an I/O request and receiving the response or completion signal. Lower response times lead to faster program execution and a more responsive system, particularly important for interactive applications.</p>
<p data-number="3"><strong>Ensure Fairness</strong>: I/O scheduling also aims to <strong>ensure fairness among different processes or users</strong> requesting I/O operations. Fairness means that no process or user should be unfairly starved of I/O service, and all requests should eventually be served in a reasonable amount of time. This prevents one process from monopolizing the I/O resources and degrading the performance of other processes.</p>
</div>
</div>
</section>
<section class="question">
<h3>4. (4 marks)</h3>
<div class="question-content">
<p>(b) With the aid of an example, describe each of the following pathnames as used in directory management.</p>
<p>(i) absolute path;</p>
<p>(ii) relative path.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Absolute Path</strong>:</p>
<p>An <strong>absolute path</strong> is a pathname that specifies the location of a file or directory starting from the root directory of the file system. It provides a complete and unambiguous path to the target, regardless of the current working directory. In Unix-like systems, it begins with a forward slash `/`, and in Windows, it typically starts with a drive letter followed by a colon and a backslash, like `C:\`.</p>
<p><strong>Example (Unix-like):</strong> `/home/user/documents/report.txt` - This absolute path starts from the root directory `/`, goes through the `home` directory, then the `user` directory, then `documents`, and finally points to the file `report.txt`.</p>
<p><strong>Example (Windows):</strong> `C:\Users\Public\Pictures\sample.jpg` - This absolute path starts from drive `C:`, goes through `Users`, then `Public`, then `Pictures`, and points to the file `sample.jpg`.</p>
<p><strong>(ii) Relative Path</strong>:</p>
<p>A <strong>relative path</strong> specifies the location of a file or directory relative to the current working directory. It does not start from the root directory. Relative paths are shorter and more convenient when working within a specific part of the file system. They are interpreted based on the current directory.</p>
<p><strong>Example (assuming current working directory is `/home/user/documents/` in Unix-like systems):</strong> `./project/data.csv` or `project/data.csv` - This relative path assumes the current directory is `/home/user/documents/`. It points to a subdirectory `project` within the current directory, and then to the file `data.csv` within `project`. `../images/logo.png` - This relative path goes up one level from the current directory (`/home/user/documents/` to `/home/user/`) and then into the `images` directory to find `logo.png`.</p>
</div>
</div>
</section>
<section class="question">
<h3>4. (4 marks)</h3>
<div class="question-content">
<p>(c) A computer system has a set of five processes P1, P2, P3, P4 and Ps on the queue and which arrive at time 0, in that order. Each of the processes has a CPU burst time as shown in Table 1. Use it to answer the questions that follow.</p>
<p><strong>Table 1</strong></p>
<table>
<thead>
<tr>
<th>Process</th>
<th>Burst time(ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td data-label="Process">P1</td>
<td data-label="Burst time(ms)">12</td>
</tr>
<tr>
<td data-label="Process">P2</td>
<td data-label="Burst time(ms)">25</td>
</tr>
<tr>
<td data-label="Process">P3</td>
<td data-label="Burst time(ms)">6</td>
</tr>
<tr>
<td data-label="Process">P4</td>
<td data-label="Burst time(ms)">9</td>
</tr>
<tr>
<td data-label="Process">P5</td>
<td data-label="Burst time(ms)">15</td>
</tr>
</tbody>
</table>
<p>Determine each of the following:</p>
<p>(i) the average waiting time, assuming non-preemptive shortest job first scheduling algorithm.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Average waiting time using Non-Preemptive Shortest Job First (SJF)</strong>:</p>
<p><strong>Process Execution Order (SJF - Non-Preemptive):</strong> P3 (6ms), P4 (9ms), P1 (12ms), P5 (15ms), P2 (25ms)</p>
<p><strong>Gantt Chart:</strong></p>
<p>[P3(6)] [P4(15)] [P1(27)] [P5(42)] [P2(67)]</p>
<p><strong>Completion Times:</strong></p>
<p>P3: 6ms</p>
<p>P4: 6 + 9 = 15ms</p>
<p>P1: 15 + 12 = 27ms</p>
<p>P5: 27 + 15 = 42ms</p>
<p>P2: 42 + 25 = 67ms</p>
<p><strong>Turnaround Times (TAT) = Completion Time - Arrival Time (Arrival Time = 0 for all processes):</strong></p>
<p>TAT(P3) = 6ms</p>
<p>TAT(P4) = 15ms</p>
<p>TAT(P1) = 27ms</p>
<p>TAT(P5) = 42ms</p>
<p>TAT(P2) = 67ms</p>
<p><strong>Waiting Times (WT) = Turnaround Time - Burst Time:</strong></p>
<p>WT(P3) = 6 - 6 = 0ms</p>
<p>WT(P4) = 15 - 9 = 6ms</p>
<p>WT(P1) = 27 - 12 = 15ms</p>
<p>WT(P5) = 42 - 15 = 27ms</p>
<p>WT(P2) = 67 - 25 = 42ms</p>
<p><strong>Average Waiting Time = (WT(P1) + WT(P2) + WT(P3) + WT(P4) + WT(P5)) / 5</strong></p>
<p>Average Waiting Time = (15 + 42 + 0 + 6 + 27) / 5 = 90 / 5 = 18ms</p>
<p><strong>The average waiting time for Non-Preemptive SJF is 18ms.</strong></p>
</div>
</div>
</section>
<section class="question">
<h3>4. (3 marks)</h3>
<div class="question-content">
<p>(ii) the waiting time for processes p1, p3 and ps assuming first come first served scheduling algorithm.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(ii) Waiting time for P1, P3, and P5 using First Come First Served (FCFS)</strong>:</p>
<p><strong>Process Execution Order (FCFS - Arrival Order):</strong> P1, P2, P3, P4, P5</p>
<p><strong>Gantt Chart:</strong></p>
<p>[P1(12)] [P2(37)] [P3(43)] [P4(52)] [P5(67)]</p>
<p><strong>Completion Times:</strong></p>
<p>P1: 12ms</p>
<p>P2: 12 + 25 = 37ms</p>
<p>P3: 37 + 6 = 43ms</p>
<p>P4: 43 + 9 = 52ms</p>
<p>P5: 52 + 15 = 67ms</p>
<p><strong>Waiting Times (WT) = Turnaround Time - Burst Time (Arrival Time = 0 for all processes):</strong></p>
<p>WT(P1) = 12 - 12 = 0ms</p>
<p>WT(P3) = 43 - 6 = 37ms</p>
<p>WT(P5) = 67 - 15 = 52ms</p>
<p><strong>Waiting time for P1 is 0ms, for P3 is 37ms, and for P5 is 52ms.</strong></p>
</div>
</div>
</section>
<section class="question">
<h3>4. (6 marks)</h3>
<div class="question-content">
<p>(d) With the aid of a schematic diagram, describe swapping technique, assuming two processes are run on a round-robin scheduling algorithm.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Swapping Technique with Round-Robin Scheduling</strong>:</p>
<p><strong>Swapping</strong> is a memory management technique used in operating systems to handle processes that exceed the available main memory (RAM). It involves moving processes between main memory and secondary storage (disk). In the context of Round-Robin scheduling with two processes (Process A and Process B), the swapping technique works as follows:</p>
<p><strong>Schematic Diagram:</strong></p>
<pre>+-----------------+     +-----------------+
|   Main Memory   |     | Secondary Storage|
+-----------------+     +-----------------+
| Process A (RAM) | &lt;---&gt; | Process B (Disk)|
|     ...         |     |     ...         |
+-----------------+     +-----------------+
    CPU
     ^
     |
+--------+
| Kernel |
+--------+
</pre>
<p><strong>Description of Swapping Process:</strong></p>
<p data-number="1"><strong>Time Quantum Allocation (Round-Robin):</strong> The Round-Robin algorithm assigns a fixed time quantum to each process. Let's say the time quantum is 'q'. Initially, Process A is loaded into main memory and gets CPU time for 'q' milliseconds. If Process A does not complete within 'q', it gets preempted.</p>
<p data-number="2"><strong>Process Swapping (Out):</strong> When Process A's time quantum expires or it needs to wait for I/O, the operating system's scheduler decides to switch to Process B. Before Process B can run, Process A is swapped out of main memory and onto secondary storage (disk). This means the entire memory image of Process A (or relevant parts) is written to a swap space on disk.</p>
<p data-number="3"><strong>Process Swapping (In):</strong> Process B, which was residing on secondary storage, is then swapped into main memory. The operating system loads the memory image of Process B from the disk into RAM. Now, Process B is ready to run.</p>
<p data-number="4"><strong>Context Switching and Execution:</strong> A context switch occurs to save the state of Process A and restore the state of Process B. The CPU then starts executing Process B for its allocated time quantum 'q'.</p>
<p data-number="5"><strong>Cycle Repetition:</strong> This process repeats. When Process B's time quantum is over, if Process A is ready to run again (and has been swapped out earlier), Process B will be swapped out to disk, and Process A will be swapped back into main memory. The cycle continues, allowing both processes to take turns executing on the CPU in a round-robin fashion, even if the total memory requirement exceeds the available RAM. The swapping mechanism allows for the execution of processes larger than the available physical memory by trading off disk I/O for memory space.</p>
</div>
</div>
</section>
<section class="question">
<h3>5. (4 marks)</h3>
<div class="question-content">
<p>(a) Consider the following system resources to answer the question that follows:</p>
<p>Loader, DRAM, driver, android, windows vista, disk, memory stick, interpreter</p>
<p>Classify the resources under either hardware or software.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Hardware Resources:</strong></p>
<p data-number="1"><strong>DRAM (Dynamic Random Access Memory)</strong>: This is a type of <strong>hardware memory</strong> used as main memory in computers.</p>
<p data-number="2"><strong>Disk</strong>: A <strong>disk</strong> (Hard Disk Drive or Solid State Drive) is a <strong>hardware storage device</strong> used for persistent data storage.</p>
<p data-number="3"><strong>Memory Stick</strong>: A <strong>memory stick</strong> (USB flash drive) is a portable <strong>hardware storage device</strong>.</p>
<p><strong>Software Resources:</strong></p>
<p data-number="1"><strong>Loader</strong>: A <strong>loader</strong> is a <strong>software program</strong> that loads programs and libraries into main memory for execution.</p>
<p data-number="2"><strong>Driver</strong>: A <strong>driver</strong> is a <strong>software program</strong> that controls a particular type of device connected to the computer, such as a disk drive or network card.</p>
<p data-number="3"><strong>Android</strong>: <strong>Android</strong> is a mobile <strong>operating system software</strong> developed by Google.</p>
<p data-number="4"><strong>Windows Vista</strong>: <strong>Windows Vista</strong> is a desktop <strong>operating system software</strong> developed by Microsoft.</p>
<p data-number="5"><strong>Interpreter</strong>: An <strong>interpreter</strong> is a <strong>software program</strong> that executes program instructions line by line (e.g., Python interpreter).</p>
</div>
</div>
</section>
<section class="question">
<h3>5. (4 marks)</h3>
<div class="question-content">
<p>(b) Distinguish between the functions of long term and short term schedulers in process management.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Long-Term Scheduler (Job Scheduler):</strong></p>
<p>The <strong>long-term scheduler</strong>, also known as the job scheduler, controls the <strong>degree of multiprogramming</strong> – the number of processes in memory. Its primary function is to <strong>select processes from a job pool and load them into memory for execution</strong>. It is invoked infrequently, typically when a process finishes, and it aims to balance the mix of processes (e.g., CPU-bound vs. I/O-bound) in the system to optimize resource utilization and system performance. The long-term scheduler influences system stability and resource allocation over longer time scales.</p>
<p><strong>Short-Term Scheduler (CPU Scheduler):</strong></p>
<p>The <strong>short-term scheduler</strong>, also known as the CPU scheduler, is responsible for <strong>selecting one process from the ready queue and allocating the CPU to it</strong>. It is invoked very frequently (milliseconds) whenever an event occurs that might lead to process switching (e.g., timer interrupts, I/O completion). Its main objective is to <strong>maximize CPU utilization and system throughput while minimizing response time and turnaround time</strong>. The short-term scheduler directly impacts the system's responsiveness and the efficiency of CPU usage on a very short time scale.</p>
<p><strong>Key Distinction:</strong> The long-term scheduler manages the flow of processes into the ready queue (memory), while the short-term scheduler manages the flow of processes onto the CPU from the ready queue.</p>
</div>
</div>
</section>
<section class="question">
<h3>5. (6 marks)</h3>
<div class="question-content">
<p>(c) Explain three functions of buffers as used in I/O device management.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Speed Mismatch Handling</strong>: Buffers are used to <strong>cope with speed mismatches between different components</strong> of a computer system, particularly between fast CPUs and slower I/O devices. For example, when data is transferred from a disk to memory, or from memory to a printer, the I/O device operates at a slower rate than the CPU and memory. A buffer acts as a temporary holding area, allowing the faster component to transfer data quickly to the buffer and continue with other tasks, while the slower component can retrieve data from the buffer at its own pace. This prevents the faster component from being stalled waiting for the slower one.</p>
<p data-number="2"><strong>Data Size Adaptation</strong>: Buffers help in <strong>adapting data sizes between producers and consumers</strong>. I/O devices and processes might produce or consume data in different sized chunks. For instance, a network interface may receive data in packets of varying sizes, while an application process might need data in fixed-size blocks. A buffer can accumulate data until a block of the required size is formed before passing it to the consumer, or it can break down large blocks of data into smaller units suitable for the producer. This simplifies data handling and processing for both sides of the communication.</p>
<p data-number="3"><strong>Support for Copy Semantics</strong>: Buffers are essential for <strong>supporting copy semantics in data transfer</strong>. When data is read from or written to a buffer, it creates a copy of the data. This is important in scenarios where the source of data might change while it's being processed or transmitted. By using a buffer, the system ensures that the consumer works with a consistent snapshot of the data, even if the original data source is modified concurrently. This is crucial for maintaining data integrity and preventing race conditions in I/O operations.</p>
</div>
</div>
</section>
<section class="question">
<h3>5. (6 marks)</h3>
<div class="question-content">
<p>(d) During large data transfers, computers use Direct Memory Access controllers to avoid burdening the main CPU with programmable I/O. Describe this procedure.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Direct Memory Access (DMA) Procedure for Large Data Transfers</strong>:</p>
<p data-number="1"><strong>DMA Request Initiation</strong>: When a process needs to perform a large data transfer (e.g., reading a large file from disk), it initiates an I/O request. Instead of the CPU handling each byte of data transfer using programmed I/O, the process signals the DMA controller to manage the data transfer directly.</p>
<p data-number="2"><strong>DMA Controller Setup</strong>: The CPU sets up the DMA transfer by providing the DMA controller with the following information: * <strong>Source Address</strong>: The starting address of the data to be transferred from the I/O device (e.g., disk buffer). * <strong>Destination Address</strong>: The starting address in main memory where the data should be written to or read from. * <strong>Transfer Length (Count)</strong>: The number of bytes or blocks to be transferred. * <strong>Transfer Mode</strong>: Read or Write operation, and other control flags.</p>
<p data-number="3"><strong>CPU Relinquishes Bus Control</strong>: Once the DMA transfer is set up, the CPU signals the DMA controller to start the transfer and relinquishes control of the system bus (address bus, data bus, and control bus). The CPU can then proceed with other tasks or processes, effectively offloading the data transfer operation.</p>
<p data-number="4"><strong>DMA Transfer Execution</strong>: The DMA controller takes over control of the system bus and directly transfers data between the I/O device and main memory without CPU intervention. For each byte or block of data, the DMA controller performs the following steps: * Requests bus access from the bus controller. * Once granted, it reads data from the source address (I/O device or memory). * Writes the data to the destination address (memory or I/O device). * Increments the source and destination addresses and decrements the transfer count.</p>
<p data-number="5"><strong>Transfer Completion and CPU Notification</strong>: The DMA controller continues the data transfer until the specified number of bytes has been transferred. Upon completion of the transfer, the DMA controller notifies the CPU by sending an interrupt signal. This interrupt signals to the CPU that the DMA transfer is complete, and the data is now available in main memory (for read operations) or has been sent to the I/O device (for write operations).</p>
<p data-number="6"><strong>CPU Resumes Operation</strong>: The CPU, upon receiving the DMA completion interrupt, can now resume processing the data that has been transferred. Because the CPU was not involved in the byte-by-byte transfer, it was free to perform other computations concurrently, significantly improving system performance, especially for large data I/O operations.</p>
</div>
</div>
</section>
<section class="question">
<h3>6. (4 marks)</h3>
<div class="question-content">
<p>(a) Outline four causes of file system failure in a computer system.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Hardware Failures</strong>: <strong>Hardware failures</strong>, particularly in storage devices like hard drives or SSDs, are a primary cause of file system failure. Disk crashes, head failures, sector errors, or controller malfunctions can lead to data corruption or loss, rendering parts or all of the file system inaccessible or inconsistent. Physical damage to storage media due to power surges, overheating, or mechanical issues can also cause failures.</p>
<p data-number="2"><strong>Software Errors</strong>: <strong>Software errors</strong> within the operating system or file system drivers can lead to corruption or inconsistencies in the file system structure. Bugs in file system implementation, improper handling of disk operations, or errors during system updates can cause metadata corruption, lost files, or directory structure damage. File system utilities themselves, if faulty, can also lead to failures during operations like formatting or repair.</p>
<p data-number="3"><strong>Power Outages and System Crashes</strong>: <strong>Abrupt power outages or system crashes</strong> while file system operations are in progress can leave the file system in an inconsistent state. If metadata updates (like directory entries, inodes, or file allocation tables) are interrupted mid-write, the file system structure on disk might become corrupted, leading to data loss or file system errors upon reboot. Unclean shutdowns are a common cause of file system problems.</p>
<p data-number="4"><strong>Human Error</strong>: <strong>Human error</strong>, such as accidental deletion of critical system files, incorrect file permissions leading to access issues, or improper use of file system utilities (e.g., formatting the wrong partition), can result in file system failures. User actions that unintentionally modify or damage file system structures can also lead to data loss or system instability. Lack of proper backup and recovery procedures can exacerbate the impact of human errors.</p>
</div>
</div>
</section>
<section class="question">
<h3>6. (2 marks)</h3>
<div class="question-content">
<p>(b) Priority scheduling algorithm is one of the techniques the operating system use in job scheduling.</p>
<p>(i) Describe one problem associated with this algorithm.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Problem with Priority Scheduling: Starvation</strong>:</p>
<p>One significant problem associated with priority scheduling is <strong>starvation</strong>. Starvation occurs when <strong>low-priority processes are indefinitely blocked from running because higher-priority processes continuously arrive and get executed</strong>. If there is a constant stream of high-priority jobs, lower-priority jobs may never get a chance to run, effectively being starved of CPU time. This can lead to unfair resource allocation and prevent essential low-priority tasks from being completed.</p>
</div>
</div>
</section>
<section class="question">
<h3>6. (4 marks)</h3>
<div class="question-content">
<p>(ii) Explain a solution to the problem in (i).</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(ii) Solution to Starvation: Aging</strong>:</p>
<p>A common solution to the starvation problem in priority scheduling is <strong>aging</strong>. <strong>Aging is a technique that gradually increases the priority of processes that have been waiting in the ready queue for a long time</strong>. Over time, even low-priority processes will have their priority increased enough to eventually become the highest priority process and get scheduled to run. This ensures that no process is indefinitely postponed and provides a form of fairness in priority scheduling. Aging can be implemented by periodically increasing the priority level of waiting processes based on their waiting time. For example, every certain time interval, the priority of each waiting process is increased by a small amount. This ensures that eventually, even the lowest priority process will have a high enough priority to be executed.</p>
</div>
</div>
</section>
<section class="question">
<h3>6. (4 marks)</h3>
<div class="question-content">
<p>(c) The first-fit strategy for memory allocation suffers from the problem of external fragmentation. Describe a solution to this problem.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Solution to External Fragmentation for First-Fit: Compaction</strong>:</p>
<p>A primary solution to the problem of external fragmentation, particularly when using first-fit memory allocation, is <strong>compaction</strong>. <strong>Compaction is a memory management technique that rearranges processes in memory to consolidate the free space into larger contiguous blocks</strong>. When external fragmentation becomes severe (i.e., many small, non-contiguous blocks of free memory), the operating system can perform compaction. This involves moving all allocated memory blocks to one end of the memory space, leaving all the free memory together in one large block at the other end.</p>
<p><strong>Compaction Process:</strong></p>
<p data-number="1"><strong>Identify Movable Blocks</strong>: The system needs to identify which memory blocks can be relocated. Typically, code and data segments of processes are relocatable.</p>
<p data-number="2"><strong>Relocate Processes</strong>: Processes are moved in memory such that they become contiguous, and all free space is grouped together. This may involve adjusting process address tables and pointers to reflect the new memory locations.</p>
<p data-number="3"><strong>Update Memory Management Data Structures</strong>: After relocation, the memory management data structures (e.g., free lists, bitmaps) are updated to reflect the new layout of allocated and free memory. Compaction effectively eliminates external fragmentation by creating larger contiguous blocks of free memory, making it possible to allocate memory for larger processes that could not be accommodated before due to fragmentation. However, compaction is a costly operation as it involves moving potentially large amounts of data in memory, which can lead to system performance overhead. It is typically performed only when memory fragmentation becomes a critical issue.</p>
</div>
</div>
</section>
<section class="question">
<h3>7. (6 marks)</h3>
<div class="question-content">
<p>(d) A system disk has been diagnosed to be having low access speed. Explain three ways in which the operating system could improve the performance of such a disk.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Disk Caching</strong>: Implementing <strong>disk caching</strong> can significantly improve disk access speed. Disk caching involves using a portion of main memory (RAM) as a cache to store frequently accessed disk blocks. When a read request comes, the operating system first checks if the requested data is in the cache. If it is (cache hit), the data is served directly from the faster RAM, avoiding the slower disk access. For write operations, write-back caching can be used, where data is initially written to the cache and later flushed to disk, allowing for faster write operations as well. Effective caching algorithms (like LRU or FIFO) can drastically reduce the average disk access time by serving a high percentage of requests from the cache.</p>
<p data-number="2"><strong>Disk Scheduling Algorithms</strong>: Employing efficient <strong>disk scheduling algorithms</strong> can optimize the order in which disk I/O requests are serviced, thereby reducing seek time and rotational latency. Algorithms like SSTF (Shortest Seek Time First), SCAN (Elevator), and C-SCAN aim to minimize the head movement across the disk surface. By servicing requests in an optimized sequence, these algorithms reduce the total time spent in disk head movements, leading to faster overall disk access and improved throughput. Selecting the appropriate scheduling algorithm based on the workload characteristics can significantly enhance disk performance.</p>
<p data-number="3"><strong>File System Optimization and Defragmentation</strong>: The operating system can improve disk performance through <strong>file system optimization and defragmentation</strong>. Over time, file systems can become fragmented, meaning that files are stored in non-contiguous blocks across the disk. This increases seek time as the disk head has to move to different physical locations to read a single file. Defragmentation tools reorganize files on the disk to make them contiguous, reducing fragmentation. Additionally, optimizing file system structures (e.g., efficient directory organization, metadata management) can also improve file access times. Regular file system maintenance, including defragmentation and error checking, can help maintain optimal disk performance.</p>
</div>
</div>
</section>
<section class="question">
<h3>7. (4 marks)</h3>
<div class="question-content">
<p>(a) Outline two examples for each of the following types of interrupts within a computer system:</p>
<p>(i) external;</p>
<p>(ii) internal.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) External Interrupts</strong>:</p>
<p data-number="1"><strong>Hardware Device Interrupts</strong>: <strong>Hardware device interrupts</strong> are generated by I/O devices to signal the CPU about an event that requires attention. Examples include a <strong>keyboard interrupt</strong>, generated when a key is pressed, signaling the CPU to process the keystroke, or a <strong>network interface card (NIC) interrupt</strong>, indicating that a network packet has arrived and needs to be processed.</p>
<p data-number="2"><strong>Timer Interrupts</strong>: <strong>Timer interrupts</strong> are generated by a hardware timer at regular intervals. These interrupts are used by the operating system for time management tasks, such as <strong>time-slicing in process scheduling</strong>, where the timer interrupt preempts the currently running process after a time quantum, or for <strong>maintaining the system clock</strong> and scheduling periodic tasks.</p>
<p><strong>(ii) Internal Interrupts (Traps)</strong>:</p>
<p data-number="1"><strong>System Calls (Software Interrupts)</strong>: <strong>System calls</strong> are <strong>software-generated interrupts</strong> that occur when a user program needs to request a service from the operating system kernel. For instance, when a program wants to <strong>open a file</strong> or <strong>allocate memory</strong>, it executes a system call instruction, which triggers an internal interrupt, transferring control to the kernel to handle the request.</p>
<p data-number="2"><strong>Exceptions (Faults and Traps)</strong>: <strong>Exceptions</strong> are internal interrupts triggered by error conditions or exceptional situations during program execution. Examples include <strong>page faults</strong>, which occur when a program tries to access a memory page that is not currently in RAM, or <strong>division by zero errors</strong>, which occur when a program attempts to divide a number by zero. These exceptions cause the CPU to transfer control to an exception handler within the operating system to manage the error condition.</p>
</div>
</div>
</section>
<section class="question">
<h3>7. (8 marks)</h3>
<div class="question-content">
<p>(b) With the aid of a diagram, describe three of the layered structures of a file system as used in operating systems.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Layered Structures of a File System</strong>:</p>
<p>File systems in operating systems are typically organized in a layered structure to manage complexity and provide modularity. Here are three common layers:</p>
<p><strong>Schematic Diagram of Layered File System:</strong></p>
<pre>+-------------------------+  &lt;-- User Applications
|     User Interface      |
+-------------------------+
             | System Calls (e.g., read(), write(), open())
             V
+-------------------------+
|  Logical File System    |  &lt;-- Directory structure, file naming, permissions
+-------------------------+
             | Logical Block Addresses, File Metadata
             V
+-------------------------+
| File Organization Module|  &lt;-- Translates logical to physical addresses, manages free space
+-------------------------+
             | Physical Block Addresses, Disk Commands
             V
+-------------------------+
|   Basic File System     |  &lt;-- Issues commands to disk driver, manages buffers and caches
+-------------------------+
             | Raw Disk Blocks
             V
+-------------------------+
|     I/O Control         |  &lt;-- Disk driver, hardware interaction
+-------------------------+  &lt;-- Disk Hardware
</pre>
<p data-number="1"><strong>Logical File System Layer</strong>: This layer is concerned with <strong>logical file operations</strong> and the <strong>user's view of the file system</strong>. It manages file metadata, such as file names, directory structures, permissions, and file attributes. It translates file operations (like `open`, `read`, `write`, `delete`) requested by user applications into a sequence of lower-level operations. This layer is responsible for maintaining the file system's logical structure and enforcing access control policies. It operates in terms of logical block addresses and file identifiers, abstracting away the physical disk layout.</p>
<p data-number="2"><strong>File Organization Module (or File System Implementation Layer)</strong>: This layer is responsible for <strong>mapping logical file blocks to physical disk blocks</strong>. It manages the physical storage space on the disk, including free space management, allocation of disk blocks to files, and keeping track of file locations on disk. It translates logical block addresses provided by the logical file system layer into physical block addresses that the disk hardware can understand. This layer implements the specific file allocation method (e.g., contiguous, linked, indexed) and is crucial for efficient disk space utilization and file access performance. It often includes buffer and cache management to optimize disk I/O.</p>
<p data-number="3"><strong>Basic File System (or I/O Control Layer)</strong>: This is the <strong>lowest layer of the file system hierarchy</strong> and directly interacts with the disk hardware through the device driver. It is responsible for <strong>issuing commands to the disk controller</strong> to perform physical I/O operations (read and write sectors). This layer handles the physical transfer of data between memory and disk. It also manages I/O buffers and caches at the hardware level to improve performance. The basic file system layer abstracts away the hardware specifics of the disk device and provides a uniform interface for higher layers to interact with the storage hardware. It is closely tied to the device driver for the specific disk hardware in use.</p>
</div>
</div>
</section>
<section class="question">
<h3>7. (4 marks)</h3>
<div class="question-content">
<p>(c) Differentiate between deadlock avoidance and deadlock prevention as used in operating systems.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Deadlock Prevention</strong>:</p>
<p><strong>Deadlock prevention</strong> aims to <strong>prevent deadlocks by ensuring that at least one of the necessary conditions for deadlock cannot hold</strong>. These conditions are mutual exclusion, hold and wait, no preemption, and circular wait. Prevention strategies work by <strong>restricting resource allocation</strong> or process behavior in a way that violates at least one of these conditions. For example, to prevent "hold and wait," a system might require processes to request all necessary resources at once before starting execution. Prevention is a proactive approach applied at the system design level to avoid deadlocks from ever occurring. However, prevention methods can often lead to lower resource utilization and reduced system throughput as they impose restrictions on resource usage.</p>
<p><strong>Deadlock Avoidance</strong>:</p>
<p><strong>Deadlock avoidance</strong> allows all four deadlock conditions to potentially exist, but it makes <strong>dynamic decisions about resource allocation to ensure that the system never enters a deadlock state</strong>. Avoidance algorithms require the operating system to have <strong>advance information about the maximum resource requirements of each process</strong>. Using this information, the system checks if granting a resource request would lead to a "safe state." A safe state is one in which there is at least one execution order of all processes that allows them all to complete without deadlock. Avoidance algorithms, like the Banker's Algorithm, dynamically examine the resource allocation state and grant resources only if the resulting state is safe. Avoidance is more flexible than prevention but requires more overhead for runtime checking and advance knowledge of process resource needs.</p>
<p><strong>Key Difference:</strong> Prevention focuses on removing the possibility of deadlock conditions statically, while avoidance focuses on making runtime decisions to keep the system in a safe state, thus preventing deadlocks dynamically.</p>
</div>
</div>
</section>
<section class="question">
<h3>7. (4 marks)</h3>
<div class="question-content">
<p>(d) Explain two circumstances under which a preemptive scheduling decision would be made by the operating system during inter-process communication.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Higher Priority Process Arrival</strong>: In a preemptive priority scheduling system, if a <strong>higher-priority process becomes ready to run</strong> (e.g., it arrives in the ready queue or becomes unblocked), the operating system will make a preemptive scheduling decision. The currently running, lower-priority process will be <strong>preempted</strong> (interrupted and its state saved), and the CPU will be allocated to the newly ready, higher-priority process. This ensures that high-priority processes get immediate attention and reduces their response time, which is crucial in real-time systems or when handling urgent tasks. Preemption based on priority is a common mechanism in many preemptive scheduling algorithms.</p>
<p data-number="2"><strong>Time Quantum Expiration in Round Robin</strong>: When using <strong>Round Robin scheduling</strong>, each process is given a fixed time quantum to execute. If a process is still running when its time quantum expires, a <strong>preemptive scheduling decision is made</strong>. The operating system preempts the currently running process, saves its state, and moves it to the back of the ready queue. Then, the next process in the ready queue is scheduled to run for its time quantum. Time quantum expiration is a fundamental trigger for preemption in Round Robin, ensuring fairness and preventing any single process from monopolizing the CPU for too long in a time-shared environment.</p>
</div>
</div>
</section>
<section class="question">
<h3>8. (4 marks)</h3>
<div class="question-content">
<p>(a) Explain two disadvantages associated with linked file allocation method.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p data-number="1"><strong>Sequential Access for Direct Access Files</strong>: <strong>Linked allocation</strong> is inefficient for direct (random) access files. To access a block in the middle or at the end of a file, it's necessary to traverse the linked list of blocks from the beginning of the file. This results in <strong>sequential access performance</strong> even when direct access is desired, leading to slow access times for applications that require random file access patterns. For example, to read the 100th block of a file, the system must read the preceding 99 blocks to follow the pointers, which is highly inefficient compared to contiguous or indexed allocation.</p>
<p data-number="2"><strong>Increased Overhead for Disk Space and I/O Operations</strong>: Linked allocation introduces <strong>overhead in terms of disk space and I/O operations</strong> due to the storage of pointers. Each block of a file must contain a pointer to the next block, which reduces the amount of space available for actual data in each block. Additionally, reading a file involves following pointers from block to block, which can require <strong>multiple disk I/O operations</strong> to fetch the pointers themselves. This overhead can decrease the effective storage capacity and increase the overall I/O time, especially for files that are highly fragmented or spread across many non-contiguous blocks. Pointer maintenance and retrieval add extra steps to file operations.</p>
</div>
</div>
</section>
<section class="question">
<h3>8. (4 marks)</h3>
<div class="question-content">
<p>(b) Distinguish between page and frame as used in memory management.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>Page</strong>:</p>
<p>A <strong>page</strong> is a <strong>fixed-size block of logical memory</strong>. It is a unit of memory as seen by the process or the virtual address space. When a program is executed, its logical address space is divided into pages of equal size. Pages are a concept within the <strong>virtual memory system</strong>. They are logical divisions and are used to organize the program's address space. The size of a page is typically a power of 2 (e.g., 4KB, 8KB) and is defined by the operating system and hardware architecture. Processes operate with pages, and the operating system manages memory in terms of pages.</p>
<p><strong>Frame</strong>:</p>
<p>A <strong>frame</strong> is a <strong>fixed-size block of physical memory (RAM)</strong>. It is a unit of memory in the physical address space. Physical memory is divided into frames of the same size as pages. Frames are the actual physical locations in RAM where pages can be loaded. When a page of a process needs to be loaded into physical memory, it is placed into a frame. Frames are part of the <strong>physical memory organization</strong>. The operating system maintains a mapping between pages (logical addresses) and frames (physical addresses) using page tables. The number of frames is determined by the amount of physical RAM available in the system.</p>
<p><strong>Key Distinction:</strong> Pages are logical units of memory in the virtual address space, while frames are physical units of memory in RAM. The operating system maps pages to frames to enable virtual memory management.</p>
</div>
</div>
</section>
<section class="question">
<h3>8. (4 marks)</h3>
<div class="question-content">
<p>(c) A deadlock can be eliminated by either of the following methods:</p>
<p>(i) aborting deadlocked processes all at once;</p>
<p>(ii) aborting one process at a time until the deadlock is eliminated.</p>
<p>Describe a disadvantage associated with each elimination method.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Aborting Deadlocked Processes All at Once - Disadvantage: Significant Work Loss</strong>:</p>
<p>The major disadvantage of <strong>aborting all deadlocked processes at once</strong> is the potential for <strong>significant work loss</strong>. When all processes involved in a deadlock are terminated abruptly, any computations they have performed up to that point are lost. If these processes have been running for a long time or have performed critical operations, restarting them from the beginning can lead to a considerable waste of CPU time, resources, and user effort. It is a drastic measure that sacrifices all progress made by the deadlocked processes.</p>
<p><strong>(ii) Aborting One Process at a Time Until Deadlock is Eliminated - Disadvantage: Repeated Overhead and Potential Starvation</strong>:</p>
<p>The disadvantage of <strong>aborting one process at a time until the deadlock is resolved</strong> is the <strong>overhead of repeatedly checking for deadlock and aborting processes</strong>. After aborting a process, the system needs to re-evaluate if the deadlock is resolved. If not, another process must be chosen and aborted, and the check is repeated. This iterative process can be <strong>computationally expensive</strong> and time-consuming. Furthermore, there's a risk of <strong>choosing the same process repeatedly for abortion</strong> if the deadlock situation is complex or if the process is frequently involved in deadlocks, potentially leading to starvation for that particular process. Selecting the 'right' process to abort to efficiently resolve the deadlock is also a non-trivial problem, and poor choices can prolong the deadlock resolution process and increase overhead.</p>
</div>
</div>
</section>
<section class="question">
<h3>8. (8 marks)</h3>
<div class="question-content">
<p>(d) Distinguish between each of the following process states as used in process control:</p>
<p>(i) running state and ready state;</p>
<p>(ii) blocked state and terminated state.</p>
</div>
<div class="answer-section">
<h4>Answer</h4>
<div class="answer-content">
<p><strong>(i) Running State vs. Ready State</strong>:</p>
<p><strong>Running State</strong>:</p>
<p>A process is in the <strong>running state</strong> when it is <strong>currently being executed by the CPU</strong>. At any given moment, on a single-processor system, there is at most one process in the running state. The process in the running state is actively using the CPU to execute its instructions. It has been selected by the short-term scheduler to be dispatched to the CPU. The transition to the running state occurs when the scheduler selects a process from the ready queue and allocates the CPU to it. A process continues in the running state until it completes its CPU burst, gets preempted, or blocks for an event.</p>
<p><strong>Ready State</strong>:</p>
<p>A process is in the <strong>ready state</strong> when it is <strong>prepared and waiting to be executed by the CPU</strong>, but the CPU is currently allocated to another process. Processes in the ready state are in memory and are eligible to run. They are waiting for their turn to be assigned to the CPU by the short-term scheduler. Processes enter the ready state after they are admitted into the system, after completing an I/O operation, or after being preempted from the running state. The ready state forms a queue (ready queue) from which the scheduler selects the next process to run based on the scheduling algorithm.</p>
<p><strong>Key Distinction (Running vs. Ready):</strong> A process in the running state is actively using the CPU, while a process in the ready state is waiting to use the CPU and is eligible to be run as soon as the CPU becomes available.</p>
<p><strong>(ii) Blocked State vs. Terminated State</strong>:</p>
<p><strong>Blocked State (Waiting State)</strong>:</p>
<p>A process is in the <strong>blocked state</strong>, also known as the waiting state, when it is <strong>waiting for some event to occur before it can proceed</strong>. This event is typically an external event, such as the completion of an I/O operation (e.g., disk read/write, network data arrival), or the release of a resource it has requested (e.g., semaphore, lock). While in the blocked state, a process cannot use the CPU, even if it is available. Processes transition to the blocked state when they initiate an operation that requires waiting, such as an I/O request or a synchronization operation. They remain in the blocked state until the event they are waiting for occurs, at which point they typically transition to the ready state.</p>
<p><strong>Terminated State (Completed State)</strong>:</p>
<p>A process is in the <strong>terminated state</strong>, also known as the completed or finished state, when it has <strong>finished its execution and is no longer active</strong>. Once a process has completed its final instruction and exited, it enters the terminated state. In this state, the process is no longer eligible to run and its resources (memory, CPU time, etc.) are typically deallocated and returned to the system. The operating system reclaims the resources held by the terminated process. From the terminated state, a process cannot transition to any other state; it is the final state of a process's lifecycle.</p>
<p><strong>Key Distinction (Blocked vs. Terminated):</strong> A process in the blocked state is temporarily inactive and waiting for an event to continue execution, while a process in the terminated state has completed its execution and is no longer active or waiting.</p>
</div>
</div>
</section>
</main></p>
<footer>©2017 The Kenya National Examinations Council</footer></div>
</div>
</div>
</article>
</section>
</div>
<div id='bottomPagination'>
<nav class="pagination noprt">
<a href="november_2018.html" class="prev"><span><span>&laquo; </span>Previous</span></a> <span class="sep">| </span><a href="july_2017.html" class="next"><span>Next<span> &raquo;</span></span></a>
</nav>
</div>
</div>
<script type="text/javascript" src="_intef_js.js"></script></body></html>